{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas, numpy and matplotlib libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "%matplotlib inline\n",
    "\n",
    "# Convert csv into dataframe. \n",
    "df = pd.read_csv('CreditRisk-6488587.csv',  keep_default_na=True, sep=',\\s+', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow showing of all columns\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Prepare a data quality report for the CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Check how many rows and columns are in the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print (rows, features)\n",
    "print(\"The number of rows and features:\",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1000 customers with 24 features associated with each customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Print the first and the last 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Convert the features to their appropriate data types (e.g., decide which features are more appropriate as continuos and which ones as categorical types). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on examining the data in a spreadsheet program, the following decision was made about the data convertions:\n",
    "\n",
    "- The 'MaxDelq2PublicRecLast12M' and 'MaxDelqEver' columns were converted to **categorical** as they contained finite set of possible values. The numerical values have special meaning, and thus are not continuous in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns containing categorical data\n",
    "categorical_columns = df[['RiskPerformance', 'MaxDelq2PublicRecLast12M', 'MaxDelqEver']].columns\n",
    "#income was chosen as category because the data contains single digit values representing income categories\n",
    "\n",
    "# Convert data type to category for these columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with descriptive statistics for all the categorical features\n",
    "df.select_dtypes(['category']).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Drop duplicate rows and columns, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine the duplicate rows\n",
    "df[\"is_duplicate\"] = df.duplicated()\n",
    "df[df.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print('Duplicate rows:', df.duplicated()[df.duplicated() == True].shape[0])\n",
    "# Check for duplicate columns\n",
    "print('Duplicate columns:',df.columns.size - df.columns.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - There are 42 duplicate rows in the dataframe, with 0 duplicate columns. Therefore, the duplicate rows will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows again\n",
    "print('Duplicate rows:', df.duplicated()[df.duplicated() == True].shape[0])\n",
    "\n",
    "print('Duplicate columns:',df.columns.size - df.columns.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'is_duplicate column'\n",
    "df = df.drop('is_duplicate',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Drop constant columns, if any.\n",
    "\n",
    "We've already seen above that categorical features don't have constant columns. Let's check continuous features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns containing continuous data\n",
    "continuous_columns = df[['ExternalRiskEstimate','MSinceOldestTradeOpen','MSinceMostRecentTradeOpen','AverageMInFile','NumSatisfactoryTrades', 'NumTrades60Ever2DerogPubRec',\n",
    "                         'NumTrades90Ever2DerogPubRec', 'PercentTradesNeverDelq', 'MSinceMostRecentDelq', 'NumTotalTrades',  \n",
    "                         'NumTradesOpeninLast12M', 'PercentInstallTrades', 'MSinceMostRecentInqexcl7days', 'NumInqLast6M',\n",
    "                         'NumInqLast6Mexcl7days', 'NetFractionRevolvingBurden', 'NetFractionInstallBurden',\n",
    "                         'NumRevolvingTradesWBalance', 'NumInstallTradesWBalance', 'NumBank2NatlTradesWHighUtilization',\n",
    "                        'PercentTradesWBalance']].columns\n",
    "\n",
    "# Print table\n",
    "df[continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non zero **std** or **standard deviation** implies that a particular feature does not contain a single constant value in all of the rows. Thus in this case, none of the continuous features are constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save your updated/cleaned data frame to a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the updated dataframe to a csv file\n",
    "df.to_csv('06488587_cleaned-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Prepare a table with descriptive statistics for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(['int64']).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Prepare a table with descriptive statistics for all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with descriptive statistics for all the categorical features\n",
    "df.select_dtypes(['category']).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Plot histograms for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of the continuous features\n",
    "df[continuous_columns].hist(figsize=(25,25))\n",
    "plt.savefig('06488587_continuous_histograms.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Plot box plots for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot box plots for all the continuous features\n",
    "pp = PdfPages('06488587_continuous_boxplots.pdf')\n",
    "\n",
    "for col in continuous_columns:\n",
    "    f = df[col].plot(kind='box', figsize=(10,5))\n",
    "    pp.savefig(f.get_figure())\n",
    "    plt.show()\n",
    "\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Plot bar plots for all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot bar charts for all categorical features\n",
    "pp = PdfPages('0648857_categorical_barcharts.pdf')\n",
    "\n",
    "for column in categorical_columns:\n",
    "    f = df[column].value_counts().plot(kind='bar', title=column, figsize=(12,10))\n",
    "    pp.savefig(f.get_figure())\n",
    "    plt.show()\n",
    "\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Quality Report discussion can be found in a pdf file called **06488587_Data_Quality_Report_Initial_Findings.pdf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Prepare a data quality plan for the cleaned CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The initial list of issues as identified in the **Data_Quality_Report_Initial_Findings.pdf**:\n",
    "\n",
    "- There are -7, -8, and -9 values in the dataframe which have special meaning, but need to be dealt with, otherwise they will affect the data.\n",
    "- Many rows have a larger value for 'NumSatisfactoryTrades' than for 'NumTotalTrades'\n",
    "- 'MaxDelq2PublicRecLast12M' has two values with equal meaning: '5' and '6' both mean unknown delinquency.\n",
    "- 'NumInqLast6M' and 'NumInqLast6Mexcl7days' both have very similar data.\n",
    "- 'NumTrades60Ever2DerogPubRec' and 'NumTrades90Ever2DerogPubRec' both have very similar data.\n",
    "- The outliers in the box plots initially appear to make sense but should be further investigated here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In addition to the problems above, some standard checks will be carried out to find any additional issues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Irregular cardinalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for irregular cardinality in categorical features. There could be same values spelled differently\n",
    "print(\"Unique values for:\\n- MaxDelqEver:\", pd.unique(df.MaxDelqEver.ravel()))\n",
    "print(\"\\n- MaxDelq2PublicRecLast12M:\", pd.unique(df.MaxDelq2PublicRecLast12M.ravel()))\n",
    "print(\"\\n- RiskPerformance:\", pd.unique(df.RiskPerformance.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are no irregular cardinalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether there are null values in the data where values would be expected\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Proposed solutions to deal with the problems identified:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data collection is an expensive and/or time consuming process, it is the first priority to always try to keep as much original data intact as possible before making any cuts/removals or modifications. Thus my decisions below are based on this principle of data preservation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **There are -7, -8, and -9 values in the dataframe:**\n",
    "These values have special meaning, but as we can gather from the data dictionary, -9 No Bureau Record or No Investigation and -8 No Usable/Valid Trades or Inquiries can be treated as missing data, as the meaning behind them doesn't affect our target. Imputation will be performed on these values if there are roughly 30% or less of these values in a row/feature, otherwise the row/feature may need to be dropped.\n",
    "\n",
    "As for -7, a new binary feature will be created to keep a record of whether a row had this particular value (Condition not Met (e.g. No Inquiries, No Delinquencies)), as it has a special and useful meaning. The -7 values will then be replaced with NaN (not a number) values, so as not to skew the data.\n",
    "\n",
    "\n",
    "2) **Many rows have a larger value for 'NumSatisfactoryTrades' than for 'NumTotalTrades':**\n",
    "\n",
    "The data will be investigated to see if the correct values can be found, and if not, the rows will be dropped.\n",
    "\n",
    "\n",
    "3) **'MaxDelq2PublicRecLast12M' has two values with equal meaning: '5' and '6' both mean unknown delinquency.**\n",
    "\n",
    "The feature will use only one value to express 'unknown delinquency', with one value being replaced by the other.\n",
    "\n",
    "4) **'NumInqLast6M' and 'NumInqLast6Mexcl7days' both have very similar data.**\n",
    "The features will be investigated, and if they are roughly 90%+ similar, one will be dropped (investigation into which will be done).\n",
    "\n",
    "5) **'NumTrades60Ever2DerogPubRec' and 'NumTrades90Ever2DerogPubRec' both have very similar data.**\n",
    "\n",
    "The features will be investigated, and if they are roughly 90%+ similar, one will be dropped (investigation into which will be done).\n",
    "\n",
    "\n",
    "6) **The outliers in the box plots initially appear to make sense but should be further investigated here.**\n",
    "\n",
    "The outliers will be investigated. If they make sense they will be kept, otherwise the otherlier rows will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying solutions to the data quality issues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **There are -7, -8, and -9 values in the dataframe:** These values have special meaning, but -9 No Bureau Record or No Investigation and -8 No Usable/Valid Trades or Inquiries can be treated as missing data, as the meaning behind them doesn't affect our target. Imputation will be performed on these values if there are roughly 30% or less of these values in a row/feature, otherwise the row/feature may need to be dropped.\n",
    "\n",
    "As for -7, a new binary feature will be created to keep a record of whether a row had this particular value (Condition not Met (e.g. No Inquiries, No Delinquencies)), as it has a special and useful meaning. The -7 values will then be replaced with NaN (not a number) values, so as not to skew the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find -7 values\n",
    "df.isin([-7]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find -8 values\n",
    "df.isin([-8]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find -9 values\n",
    "df.isin([-9]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two rows which have minus -9 values for each feature. They will be found, and then dropped, as they are not useful for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ExternalRiskEstimate'] == -9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the rows\n",
    "df.drop([df.index[13], df.index[50]], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make sure there are no more -9 values\n",
    "df.isin([-9]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a new column called conditionNotMet to record -7 values.\n",
    "df['ConditionNotMet'] = df.isin([-7]).any(1).astype('category')\n",
    "# Replace with NaN\n",
    "df.replace([-7],np.NaN, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a new column to check the number of -8 values per row.\n",
    "df['NumMinus8']=df.T.isin([-8]).sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 26 features, but one is the target, one is 'NumMinus8', and one is 'ConditionNotMet', so 23 features remain. \n",
    "#### If there are 11 or more -8 values in a row (50% of the values), we will drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['NumMinus8'] >= 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No rows need to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['NumMinus8']==df['NumMinus8'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max value is 7 -8s in 23 features, so roughly 30%. We will use imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get continuous features again after -9s and -7s have been removed.\n",
    "\n",
    "new_continuous_columns = df[['ExternalRiskEstimate','MSinceOldestTradeOpen','MSinceMostRecentTradeOpen','AverageMInFile','NumSatisfactoryTrades', \n",
    "                         'NumTrades90Ever2DerogPubRec', 'PercentTradesNeverDelq', 'NumTotalTrades',  \n",
    "                         'NumTradesOpeninLast12M', 'PercentInstallTrades', 'MSinceMostRecentInqexcl7days',\n",
    "                         'NumInqLast6Mexcl7days', 'NetFractionRevolvingBurden', 'NetFractionInstallBurden',\n",
    "                         'NumRevolvingTradesWBalance', 'NumInstallTradesWBalance', 'NumBank2NatlTradesWHighUtilization',\n",
    "                        'PercentTradesWBalance', 'MSinceMostRecentDelq']].columns\n",
    "\n",
    "# Print table\n",
    "df[new_continuous_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean with -8 values remaining\n",
    "df[new_continuous_columns].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mean with -8 values replaced with NaN, for more accurate values.\n",
    "df[new_continuous_columns].replace([-8],np.NaN,).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[new_continuous_columns].isin([-8]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The -8 values will undergo imputation. No features need to be removed as the highest amount of missing values is 31% in 'NetFractionInstallBurden'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean or median depending on distribution. Mean is used where there is an existing normal\n",
    "# distribution. http://datamining.togaware.com/survivor/Mean_Median_Mode.html\n",
    "df['MSinceOldestTradeOpen'] = df['MSinceOldestTradeOpen'].replace([-8],np.NaN)\n",
    "df['MSinceOldestTradeOpen'] = df['MSinceOldestTradeOpen'].replace([np.NaN], df['MSinceOldestTradeOpen'].mean())\n",
    "\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace([-8], df['MSinceMostRecentInqexcl7days'].median())\n",
    "df['NetFractionRevolvingBurden'] = df['NetFractionRevolvingBurden'].replace([-8], df['NetFractionRevolvingBurden'].median())\n",
    "df['NetFractionInstallBurden'] = df['NetFractionInstallBurden'].replace([-8], df['NetFractionInstallBurden'].median())\n",
    "df['NumRevolvingTradesWBalance'] = df['NumRevolvingTradesWBalance'].replace([-8], df['NumRevolvingTradesWBalance'].median())\n",
    "df['NumInstallTradesWBalance'] = df['NumInstallTradesWBalance'].replace([-8], df['NumInstallTradesWBalance'].median())\n",
    "df['NumBank2NatlTradesWHighUtilization'] = df['NumBank2NatlTradesWHighUtilization'].replace([-8], df['NumBank2NatlTradesWHighUtilization'].median())\n",
    "df['PercentTradesWBalance'] = df['PercentTradesWBalance'].replace([-8], df['PercentTradesWBalance'].median())\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([-8], df['MSinceMostRecentDelq'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all -8 values are gone.\n",
    "df[new_continuous_columns].isin([-8]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the 'NumMinus8' column\n",
    "df = df.drop('NumMinus8',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check min values for minus values.\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Many rows have a larger value for 'NumSatisfactoryTrades' than for 'NumTotalTrades':**\n",
    "\n",
    "The data will be investigated to see if the correct values can be found, and if not, the rows will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['NumSatisfactoryTrades','NumTotalTrades']].sort_values('NumTotalTrades').head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the rows where 'NumSatisfactoryTrades' is greater than 'NumTotalTrades'\n",
    "df[df['NumSatisfactoryTrades'] > df['NumTotalTrades']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Take row 27 for example. \n",
    "#### MaxDelqEver = 8, which means \"current and never delinquent\".\n",
    "#### NumSatisfactoryTrades = 16\n",
    "#### PercentTradeNeverDelq = 100\n",
    "#### NumTotalTrades = 6\n",
    "\n",
    "#### We can see that the MaxDelqEver and PercentTradeNeverDelq match up, but the issue is with the other two values. If we  knew one of either NumSatisfactoryTrades or NumTotalTrades was the true value, then the other could be replaced. For example, if NumSatisfactoryTrades was confirmed to be 16, and PercentTradeNeverDelq = 100, then NumTotalTrades would equal 16. However, if NumTotalTrades was confirmed as 6, then NumSatisfactoryTrades would be replaced with 6. There is no way to determine which is correct, and as only 67 rows are affected, the decision to drop this data has been made. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['NumSatisfactoryTrades'] > df['NumTotalTrades']].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['NumSatisfactoryTrades'] > df['NumTotalTrades']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **'MaxDelq2PublicRecLast12M' has two values with equal meaning: '5' and '6' both mean unknown delinquency.**\n",
    "\n",
    "The feature will use only one value to express 'unknown delinquency', with one value being replaced by the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MaxDelq2PublicRecLast12M'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5 and 6 have equal meaning, so it will be set them to one value.\n",
    "df['MaxDelq2PublicRecLast12M'].replace(5, 6, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **'NumInqLast6M' and 'NumInqLast6Mexcl7days' both have very similar data.** \n",
    "\n",
    "The features will be investigated, and if they are roughly 90%+ similar, one will be dropped (investigation into which will be done).\n",
    "\n",
    "5) **'NumTrades60Ever2DerogPubRec' and 'NumTrades90Ever2DerogPubRec' both have very similar data.**\n",
    "\n",
    "The features will be investigated, and if they are roughly 90%+ similar, one will be dropped (investigation into which will be done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for correlation\n",
    "df.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'NumInqLast6M' will be dropped, as 'NumInqLast6Mexcl7days'\texcludes the last 7 days removes inquiries that are likely due to price comparision shopping, thus giving potentially more accurate data, and there is a 99% correlation between the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 99%\n",
    "df = df.drop('NumInqLast6M', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'NumTrades60Ever2DerogPubRec' will be dropped due an 88% correlation with 'NumTrades90Ever2DerogPubRec'. As 'NumTrades90Ever2DerogPubRec' captures the worse scenario of a customer being 90 days above late on payments, it will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 88%\n",
    "df = df.drop('NumTrades60Ever2DerogPubRec', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) **The outliers in the box plots initially appear to make sense but should be further investigated here.**\n",
    "\n",
    "The outliers will be investigated. If they make sense they will be kept, otherwise the otherlier rows will be removed.\n",
    "\n",
    "The columns with outliers remaining after the data problem solutions above are:\n",
    "\n",
    "- MSinceOldestTradeOpen\n",
    "- MSinceMostRecentTradeOpen\n",
    "- AverageMInFile\n",
    "- NumSatisfactoryTrades\n",
    "- NumTrades90Ever2DerogPubRec\n",
    "- PercentTradesNeverDelq\n",
    "- MSinceMostRecentDelq\n",
    "- NumTotalTrades\n",
    "- NumTradesOpeninLast12M\n",
    "- PercentInstallTrades\n",
    "- MSinceMostRecentInqexcl7days\n",
    "- NumInqLast6Mexcl7days\n",
    "- NetFractionRevolvingBurden\n",
    "- NumRevolvingTradesWBalance\n",
    "- NumInstallTradesWBalance\n",
    "- NumBank2NatlTradesWHighUtilization\n",
    "- PercentTradesWBalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='MSinceMostRecentTradeOpen', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')[['MSinceMostRecentTradeOpen','NumTotalTrades', 'MSinceMostRecentInqexcl7days', 'MSinceOldestTradeOpen', 'AverageMInFile', 'ExternalRiskEstimate', 'NumSatisfactoryTrades', 'NumTrades90Ever2DerogPubRec', 'PercentTradesNeverDelq', 'NumTotalTrades', 'NumTradesOpeninLast12M', 'NumInqLast6Mexcl7days']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='MSinceMostRecentTradeOpen', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')[['MSinceMostRecentTradeOpen','NumTotalTrades', 'MSinceMostRecentInqexcl7days', 'MSinceOldestTradeOpen', 'AverageMInFile', 'NumTradesOpeninLast12M']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='PercentTradesWBalance', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')[['NetFractionRevolvingBurden', 'NetFractionInstallBurden','NumRevolvingTradesWBalance', 'NumBank2NatlTradesWHighUtilization', 'NumInstallTradesWBalance']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the outliers, there doesn't seem to be anything obviously out of the ordinary; all data values seem to be possible. However, there are so many outliers that it is difficult to ascertain for certain whether or not a specific outlier is wrong or not. For these reasons, the outliers will be left intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of data quality plan:\n",
    "   \n",
    "| Feature                 | Data Quality Issue   | Handling Strategy            |\n",
    "|-------------------------|----------------------|------------------------------|\n",
    "| MSinceMostRecentInqexcl7days   | -7 special value | Set -7s to NaN, and record -7 presence in new binary column  |                |\n",
    "| MSinceMostRecentDelq   | -7 special value | Set -7s to NaN, and record -7 presence in new binary column  |\n",
    "| MSinceOldestTradeOpen   | -8 special value | Imputation |\n",
    "| MSinceMostRecentDelq   | -8 special value | Imputation |\n",
    "| MSinceMostRecentInqexcl7days | -8 special value | Imputation |\n",
    "| NetFractionRevolvingBurden  | -8 special value | Imputation |\n",
    "| NetFractionInstallBurden   | -8 special value | Imputation |\n",
    "| NumRevolvingTradesWBalance   | -8 special value | Imputation |\n",
    "| NumInstallTradesWBalance    | -8 special value | Imputation |\n",
    "| NumBank2NatlTradesWHighUtilization   | -8 special value | Imputation |\n",
    "| PercentTradesWBalance   | -8 special value | Imputation |\n",
    "| All features except RiskPerformance target feature  | -9 special values | Remove affected rows |\n",
    "| NumSatisfactoryTrades  | Higher values than NumTotalTrades| Remove affected rows  |\n",
    "| MaxDelq2PublicRecLast12M | Special values with same meaning | Replace value |\n",
    "| NumTrades60Ever2DerogPubRec | Redundancy with NumTrades90Ever2DerogPubRec| Remove column                |\n",
    "| NumInqLast6M             | Rendundancy with NumInqLast6Mexcl7days | Remove  column        |\n",
    "| MSinceOldestTradeOpen                 | Outliers             | Do nothing                   |\n",
    "| MSinceMostRecentTradeOpen      | Outliers             | Do nothing                   |\n",
    "| AverageMInFile       | Outliers             | Do nothing                   |\n",
    "| NumSatisfactoryTrades            | Outliers             | Do nothing                   |\n",
    "| NumTrades90Ever2DerogPubRec    | Outliers             | Do nothing                   |\n",
    "|PercentTradesNeverDelq     | Outliers             | Do nothing                   |\n",
    "| MSinceMostRecentDelq         | Outliers             | Do nothing                   |\n",
    "| NumTotalTrades             | Outliers             | Do nothing                   |\n",
    "| NumTradesOpeninLast12M              | Outliers             | Do nothing                   |\n",
    "| PercentInstallTrades     | Outliers             | Do nothing                   |\n",
    "| MSinceMostRecentInqexcl7days         | Outliers             | Do nothing                   |\n",
    "| NumInqLast6Mexcl7days | Outliers           | Do nothing                   |\n",
    "| NetFractionRevolvingBurden                | Outliers             | Do nothing                   |\n",
    "| NumRevolvingTradesWBalance | Outliers          | Do nothing                   |\n",
    "|NumInstallTradesWBalance       | Outliers             | Do nothing                   |\n",
    "| NumBank2NatlTradesWHighUtilization | Outliers          | Do nothing                   |\n",
    "| PercentTradesWBalance   | Outliers             | Do nothing                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with descriptive statistics for all the continuous features\n",
    "df.select_dtypes(['float64', 'int64']).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table with descriptive statistics for all the categorical features\n",
    "df.select_dtypes(['category']).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a csv file\n",
    "df.to_csv('06488587_PostDataQualityPlan-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Exploring relationships between feature pairs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that I will look at for pairwise feature interaction are the following continuous features:\n",
    "- ExternalRiskEstimate\n",
    "- PercentTradesNeverDelq\n",
    "- NumTrades90Ever2DerogPubRec\n",
    "- NumSatisfactoryTrades\n",
    "- MSinceMostRecentDelq\n",
    "- NumBank2NatlTradesWHighUtilization\n",
    "- NetFractionRevolvingBurden\n",
    "- NetFractionInstallBurden\n",
    "- NumInstallTradesWBalance\n",
    "- NumRevolvingTradesWBalance\n",
    "- NumTotalTrades\n",
    "- PercentTradesWBalance\n",
    "- NumTradesOpeninLast12M\n",
    "- PercentInstallTrades\n",
    "- AverageMInFile\n",
    "- MSinceOldestTradeOpen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "and the following categorical features:\n",
    "- MaxDelqEver\n",
    "- MaxDelq2PublicRecLast12M\n",
    "- RiskPerformance\n",
    "\n",
    "The choices here I based on the idea that these particular features can affect the RiskPerformance target feature, due to the reasoning that type and number of trades, length of time of trades, delinquencies, and trades with balances are all highly likely to affect the risk.\n",
    "\n",
    "### Correlations for the numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Correlation matrix using code found on https://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Select columns containing continuous data\n",
    "continuous_columns = df[['ExternalRiskEstimate', 'PercentTradesNeverDelq', 'NumTrades90Ever2DerogPubRec', \n",
    "'NumSatisfactoryTrades', 'MSinceMostRecentDelq', 'NumBank2NatlTradesWHighUtilization','NetFractionRevolvingBurden', \n",
    "'NetFractionInstallBurden', 'NumInstallTradesWBalance', 'NumRevolvingTradesWBalance', 'NumTotalTrades', 'PercentTradesWBalance',\n",
    "    'NumTradesOpeninLast12M', 'PercentInstallTrades', 'AverageMInFile', 'MSinceOldestTradeOpen']].columns\n",
    "\n",
    "# Calculate correlation of all pairs of continuous features\n",
    "corr = df[continuous_columns].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "# Generate a custom colormap - blue and red\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "            square=True, xticklabels=True, yticklabels=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "plt.yticks(rotation = 0)\n",
    "plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells are the plots for pairs of continuous features which have been shown to have a high correlation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='PercentTradesNeverDelq', y='ExternalRiskEstimate')\n",
    "df.plot(kind='scatter', x='NumSatisfactoryTrades', y='NumTotalTrades')\n",
    "df.plot(kind='scatter', x='NumBank2NatlTradesWHighUtilization', y='NumRevolvingTradesWBalance')\n",
    "df.plot(kind='scatter', x='NumBank2NatlTradesWHighUtilization', y='NetFractionRevolvingBurden')\n",
    "df.plot(kind='scatter', x='PercentTradesWBalance', y='NetFractionRevolvingBurden')\n",
    "df.plot(kind='scatter', x='AverageMInFile', y='MSinceOldestTradeOpen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots it is apparent that the stronger the correlation (the closer to 1.0 in the heat map), the more of a linear correlation occurs. The average months in file grows linearly as the months since the oldest trade open increases for example. The number of satisfactory trades verus total number of trades has the highest correlation, and thus is the most linear. This shows that the vast majority of trades are satisfactory, and thus delinquencies are low.\n",
    "\n",
    "We can also see that the percentage of trades with balance is related to the net fraction revolving burden, showing that revolving trades are generally riskier than installment trades. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical feature plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MaxDelq2PublicRecLast12M = pd.unique(df.MaxDelq2PublicRecLast12M.ravel())\n",
    "dfnew = df.copy()\n",
    "dfnew['percent'] = 0\n",
    "\n",
    "for i in MaxDelq2PublicRecLast12M:\n",
    "    count = 1 / dfnew[dfnew.MaxDelq2PublicRecLast12M == i].count()['RiskPerformance']\n",
    "    index_list = dfnew[dfnew['MaxDelq2PublicRecLast12M'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        dfnew.loc[ind, 'percent'] = count * 100\n",
    "        \n",
    "group = dfnew[['percent','MaxDelq2PublicRecLast12M','RiskPerformance']].groupby(['MaxDelq2PublicRecLast12M','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"RiskPerformance based on MaxDelq2PublicRecLast12M\", figsize=(15,7))\n",
    "\n",
    "red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "my_plot.set_xlabel(\"MaxDelq2PublicRecLast12M\")\n",
    "my_plot.set_ylabel(\"RiskPerformance\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results of this bar plot are largely in line with what we would expect: values 0 - 4 means delinquency, 6 means unknown delinquency, and 7 means current and never delinquent. Therefore unsurprisingly, 7 has the highest amount of 'Good' outcomes. \n",
    "\n",
    "#### One interesting point is how 90 days delinquent (bar 2) always results in a 'Bad' outcome, whereas 120+ days delinquent (bar 1), which we would expect to be worse, actually has more 'Good' outcomes. This shows that there are other factors at play when deciding a 'Good' or 'Bad' outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxDelqEver = pd.unique(df.MaxDelqEver.ravel())\n",
    "dfnew = df.copy()\n",
    "dfnew['percent'] = 0\n",
    "\n",
    "for i in MaxDelqEver:\n",
    "    count = 1 / dfnew[dfnew.MaxDelqEver == i].count()['RiskPerformance']\n",
    "    index_list = dfnew[dfnew['MaxDelqEver'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        dfnew.loc[ind, 'percent'] = count * 100\n",
    "        \n",
    "group = dfnew[['percent','MaxDelqEver','RiskPerformance']].groupby(['MaxDelqEver','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"RiskPerformance based on MaxDelqEver\", figsize=(15,7))\n",
    "\n",
    "red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "my_plot.set_xlabel(\"MaxDelqEver\")\n",
    "my_plot.set_ylabel(\"RiskPerformance\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, bar 3 which is 120+ days delinquent has the highest 'Bad' outcome, whereas bar 8 which is current and never delinquent has the highest 'Good' outcome, as we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous-categorical feature plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "flierprops = dict(marker='o', markerfacecolor='green', markersize=6,\n",
    "                  linestyle='none')\n",
    "\n",
    "df.boxplot(column=['ExternalRiskEstimate'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "df.boxplot(column=['MSinceOldestTradeOpen'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "df.boxplot(column=['AverageMInFile'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "df.boxplot(column=['NumSatisfactoryTrades'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "df.boxplot(column=['PercentTradesNeverDelq'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "df.boxplot(column=['NumRevolvingTradesWBalance'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "\n",
    "df.boxplot(column=['NumBank2NatlTradesWHighUtilization'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "df.boxplot(column=['PercentTradesWBalance'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "df.boxplot(column=['MSinceMostRecentDelq'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "df.boxplot(column=['NumTotalTrades'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "df.boxplot(column=['NetFractionInstallBurden'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "df.boxplot(column=['NetFractionRevolvingBurden'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['ExternalRiskEstimate']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['ExternalRiskEstimate']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['MSinceOldestTradeOpen']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['MSinceOldestTradeOpen']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['AverageMInFile']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['AverageMInFile']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['NumSatisfactoryTrades']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['NumSatisfactoryTrades']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['PercentTradesNeverDelq']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['PercentTradesNeverDelq']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['NetFractionRevolvingBurden']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['NetFractionRevolvingBurden']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['NumBank2NatlTradesWHighUtilization']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['NumBank2NatlTradesWHighUtilization']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['PercentTradesWBalance']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['PercentTradesWBalance']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RiskPerformance'] == 'Good'][['MSinceMostRecentDelq']].hist(figsize=(7,7), bins=40)\n",
    "df[df['RiskPerformance'] == 'Bad'][['MSinceMostRecentDelq']].hist(figsize=(7,7), bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings:\n",
    "\n",
    "From the continuous - continuous data visualization, we can see that most \n",
    "trades carried out result in a satisfactory outcome. There is also a strong link between\n",
    "months since oldest trade open and average month in file, which makes sense as increase as time\n",
    "progresses. \n",
    "\n",
    "We can also see that Number Bank/Natl Trades w high utilization ratio\n",
    "and revolving trades are strongly correlated, and not installment trades. This signifies that Bank/Natl\n",
    "trades with balance are most likely going to be revolving trades.\n",
    "\n",
    "Net fraction revolving burden is correlated highly with percentage of trades with balance, which\n",
    "seems to be pointing at revolving trades being the riskier trade type.\n",
    "\n",
    "With categorical - categorical, rather unsurprisingly we see that those who avoid delinquency tend to have better chance at \n",
    "having a 'Good' outcome.\n",
    "\n",
    "In categorical - continuous, a higher external risk estimate is indicative of a good risk performance outcome, with a larger \n",
    "number of months since most recent delinquency also serving as an indictor of a 'Good' outcome.\n",
    "\n",
    "Higher values of net fraction revolving burden and percentage trades with balance are very strong indicators for \n",
    "a 'Bad' outcome. Interestingly, the number of satisfactory trades is more loosely linked\n",
    "to a 'Good' outcome, showing that present revolving burden and trades with balance outweigh the\n",
    "past 'goodwill' from previous satisfactory outcomes.\n",
    "\n",
    "These aforementioned features are potential candidates for a predictive model of risk performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform, extend or combine the existing features to create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Something which the data highlighted is that the longer a person has been trading, the more likely there was to be a 'Good' outcome. However, the months since the oldest trade open and months since the most recent trade open were not looked at in combination in our data which is odd, as 'MSinceOldestTradeOpen' - 'MSinceMostRecentTradeOpen' gives the months within which the customer was trading. A customer could have a very old MSinceOldestTradeOpen, but have stopped trading a month later.  As seen from the boxplot below, the longer total months trading, from first to last, means a better chance at a 'Good' outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MonthsTrading'] = df['MSinceOldestTradeOpen'] - df['MSinceMostRecentTradeOpen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.boxplot(column=['MonthsTrading'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the data seen so far, a high net fraction burden is a good indicator of a 'Bad' outcome. While NetFractionInstallBurden has the greatest sway on the target feature, NetFractionInstallBurden is also an indicator for the target outcome. Therefore, adding the two fractions together gives a feature with a strong correlation towards the target feature; the higher the total net fraction burden, the more likely a 'Bad' outcome will occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalNetFractionBurden'] = df['NetFractionInstallBurden'] + df['NetFractionRevolvingBurden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.boxplot(column=['TotalNetFractionBurden'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From looking at the data, it can be seen that the number of satisfactory trades does not always equal to the total number of trades minus the delinquent trades. As delinquency starts at 30 days in the data dictionary, it can be assumed that any late payments below 30 days aren't counted as delinquent, and are still counted as satisfactory. Thus, a percentage of satisfactory trades feature can more accurately capture the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PercentSatisTrades'] = (df['NumSatisfactoryTrades']*100) / df['NumTotalTrades']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many of the features such as 'MSinceMostRecentInqexcl7days' have large 'tails' (exponential distribution) and thus a large amount of outliers.  Many machine learning algorithms benefit from having such data normalised. \n",
    "\n",
    "#### \"Having continuous features in an ABT that cover very different ranges can cause difficulty for some machine learning algorithms.....Normalization techniques can be used to change a continuous feature to fall within a specified range while maintaining the relative differences between the values for the feature.\" - Fundamentals of Machine Learning For Predictive Data Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MSinceMostRecentInqexcl7days'] = (df['MSinceMostRecentInqexcl7days'] - df['MSinceMostRecentInqexcl7days'].mean())/df['MSinceMostRecentInqexcl7days'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['MSinceMostRecentInqexcl7days'].hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('06488587_added_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
