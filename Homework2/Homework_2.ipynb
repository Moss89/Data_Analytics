{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task:\n",
    "\n",
    "### This homework is focused on using and evaluating three predictive models: Linear Regression, Logistic Regression, and Random Forests. The dataset comes from a credit scoring company which is interested in the reduction of credit repayment risk. The dataset  is comprised of 26 columns, and 840 rows, with the following data provided:\n",
    "\n",
    "#####  - RiskPerformance\n",
    "#####  - ExternalRiskEstimate                  \n",
    "#####  - MSinceOldestTradeOpen                 \n",
    "#####  - MSinceMostRecentTradeOpen              \n",
    "#####  - AverageMInFile                         \n",
    "#####  - NumSatisfactoryTrades                  \n",
    "#####  - NumTrades90Ever2DerogPubRec            \n",
    "#####  - PercentTradesNeverDelq                 \n",
    "#####  - MSinceMostRecentDelq                  \n",
    "#####  - MaxDelq2PublicRecLast12M                \n",
    "#####  - MaxDelqEver                             \n",
    "#####  - NumTotalTrades                          \n",
    "#####  - NumTradesOpeninLast12M                  \n",
    "#####  - PercentInstallTrades                    \n",
    "#####  - MSinceMostRecentInqexcl7days          \n",
    "#####  - NumInqLast6Mexcl7days                   \n",
    "#####  - NetFractionRevolvingBurden              \n",
    "#####  - NetFractionInstallBurden                \n",
    "#####  - NumRevolvingTradesWBalance              \n",
    "#####  - NumInstallTradesWBalance                \n",
    "#####  - NumBank2NatlTradesWHighUtilization      \n",
    "#####  - PercentTradesWBalance \n",
    "##### - DelqEver                               \n",
    "##### - DelqLast12M                             \n",
    "##### - PercentSatisfactoryTrades             \n",
    "##### - NumTradesWBalance      \n",
    "\n",
    "\n",
    "### The target feature is RiskPerformance, which has two possible outcomes: Good and Bad. The goal of this work is to find a subset of features which have a correlation with the target feature, and then use this subset to create and evaluate predictive models in an effort to see if the models can accurately predict the target feature. If the models are successful, then they will help the company to judge which of their customers will be able to repay their credit within a 2 year period.\n",
    "\n",
    "##### The data used in this homework has been sourced from: https://community.fico.com/s/explainable-machine-learning-challenge?tabset-3158a=2\n",
    "\n",
    "##### This data used for this homework has been cleaned as follows:\n",
    "\n",
    "- Duplicate rows and constant columns have been dropped.\n",
    "- Null and irregular cardinalities were checked for: none found.\n",
    "- Rows with a value of -9 for every feature were dropped.\n",
    "- Rows with -8 values were subject to imputation, except for NetFractionInstallBurden where they were set to null.\n",
    "- -7 values were set to be 1.5 * (largest value) of the feature which contained the -7.\n",
    "- Rows where the value of NumSatisfactoryTrades was greater NumTotalTrades were dropped.\n",
    "- MaxDelq2PublicRecLast12M had two equal values where 5 and 6 meant unknown delinquency. Thus, all 5 values were replaced with 6 values.\n",
    "- NumInqLast6M and NumInqLast6Mexcl7days both had very similar data. NumInqLast6M was dropped.\n",
    "- NumTrades60Ever2DerogPubRec and NumTrades90Ever2DerogPubRec both had very similar data. NumTrades60Ever2DerogPubRec was dropped.\n",
    "- Rows where NumTradesOpeninLast12M was greater than NumTotalTrades were dropped.\n",
    "- Rows where NumTrades90Ever2DerogPucRec was greater than NumTotalTrades were dropped.\n",
    "- Rows where MSinceMostRecentTradeOpen was greater than MSinceOldestTradeOpen were dropped.\n",
    "- Rows where NumRevolvingTradesWBalance was greater than NumTotalTrades were dropped.\n",
    "- Rows where NumInstallTradesWBalance was greater than NumTotalTrades were dropped.\n",
    "- Rows where NumBank2NatlTradesWHighUtilization was greater than NumTotalTrades were dropped.\n",
    "- Rows where NumTotalTrades was greater than MSinceMoatRecentTradeOpen were dropped.\n",
    "- Rows where PercentTradesNeverDelq was 100% and NumTrades60Ever2DerogPubRec had positive entries were dropped.\n",
    "- Rows where PercentTradesNeverDelq was less than 100% and NumTrades60Ever2DerogPubRec was 0 were dropped.\n",
    "- Rows where MSinceMostRecentDelq was equal to 0 and NumTrades60Ever2DerogPubRec was 0 were dropped.\n",
    "- Outliers were examined and deemed to be fit to remain.\n",
    "- Replaced MaxDelq2PublicRecLast12M with MaxDelqEver scale.\n",
    "- Combined 7 & 8 values in MaxDelq2PublicRecLast12M\tand MaxDelqEver. \n",
    "- New feature created: DelqEver, which measures if a entry has ever been delinquent.\n",
    "- New feature created: DelqLast12M which measures if a entry during the last 12 months has ever been delinquent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, the prerequisite software tools are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from patsy import dmatrices\n",
    "\n",
    "\n",
    "# For shuffling the dataframe\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# For creating the models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cleaned CSV is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CreditRisk-H2.csv')\n",
    "\n",
    "# Print the first 10 rows of the dataframe. \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numbers of rows and columns in the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 840 rows, and 26 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the data types of the columns within the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'MaxDelqEver', 'MaxDelq2PublicRecLast12M', 'RiskPerformance', 'DelqEver' and 'DelqLast12M' are all categorical features. 'DelqEver' and 'DelqLast12M' contain boolean values, and not continuous values. 'MaxDelqEver' and 'MaxDelq2PublicRecLast12M' are numerical values, but these numbers have set meaning, and are not continuous features. 'RiskPerformance' has the binary values of 'Good' and 'Bad', and is not a continuous feture. Set these as categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MaxDelqEver'] = df['MaxDelqEver'].astype('category')\n",
    "df['MaxDelq2PublicRecLast12M'] = df['MaxDelq2PublicRecLast12M'].astype('category')\n",
    "df['RiskPerformance'] = df['RiskPerformance'].astype('category')\n",
    "df['DelqEver'] = df['DelqEver'].astype('category')\n",
    "df['DelqLast12M'] = df['DelqLast12M'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The target feature is 'RiskPerformance', and thus for our predictive models, it must be marked as separate from the rest of the features. X will refer to all of the features in the dataframe excluding the target, and Y will refer to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[x for x in df.columns.values if x not in ['RiskPerformance']]]\n",
    "y = df.RiskPerformance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to ensure that the same data is being used each time this notebook is run, the following code is commented out as it is required to be run just once. The output is stored in 2 separate CSV files named 'training.csv' and 'test.csv' which are imported every time the notebook is run. This will ensure that every user of this notebook will have the same data in the training and test dataframes (explained below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffle the dataframe, and print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = shuffle(df)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When training and evaluating predicitive models, it is important to split the data between training data and test data. This way, the test data is something which the model hasn't trained on, and therefore removes the possibility that the model has just \"learned\" the data provided. As the test data will be provided after the model has been trained, a better picture of the performance of the model on new, unseen data can be obtained.\n",
    "\n",
    "##### Here, the data is split 70% as training data, 30% as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two new dataframes are created, one comprised of only the training data, and one only comprised of the test data. The original dataframe has now been separated into two different dataframes, with the test data being set aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf=pd.concat([X_train, y_train], axis=1)\n",
    "#testdf=pd.concat([X_test, y_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dataframes are saved to CSV files for importing whenever this notebook is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf.to_csv('training.csv', index=False)\n",
    "#testdf.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the training and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv('training.csv')\n",
    "testdf = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.select_dtypes(['float64', 'int64']).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are now 588 rows in the training dataframe, which is 70% of the original dataframe. There are no minus values present, as expected from the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select only the continuous features from the training dataframe, so as to allow for the upcoming continuous feature correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_continuous_features = traindf.select_dtypes(['float64', 'int64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the correlation between the continuous features. A 1:1 correlation is signified by a 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_continuous_features.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An example of a high correlation is between 'NumBank2NatlTradesWHighUtilization' and 'NumRevolvingTradesWBalance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[['NumBank2NatlTradesWHighUtilization', 'NumRevolvingTradesWBalance']].corr().as_matrix()[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To allow for easier analysis of correlation, a correlation matrix is created, which shows correlation between continuous features in illustated form. All continuous features from the training dataframe are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "# Select columns containing continuous data\n",
    "continuous_columns = traindf[['ExternalRiskEstimate', 'PercentTradesNeverDelq', 'NumTrades90Ever2DerogPubRec', \n",
    "'NumSatisfactoryTrades', 'MSinceMostRecentDelq', 'NumBank2NatlTradesWHighUtilization','NetFractionRevolvingBurden', \n",
    "'NetFractionInstallBurden', 'MSinceMostRecentTradeOpen', 'NumInstallTradesWBalance', 'NumRevolvingTradesWBalance', \n",
    "                              'NumTotalTrades', 'PercentTradesWBalance', 'MSinceMostRecentInqexcl7days', \n",
    "                              'NumInqLast6Mexcl7days', 'MSinceOldestTradeOpen',\n",
    "    'NumTradesOpeninLast12M', 'PercentInstallTrades', 'AverageMInFile', 'PercentSatisfactoryTrades',\n",
    "                             'NumTradesWBalance']].columns\n",
    "\n",
    "# Calculate correlation of all pairs of continuous features\n",
    "corr = traindf[continuous_columns].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "# Generate a custom colormap - blue and red\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "            square=True, xticklabels=True, yticklabels=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "plt.yticks(rotation = 0)\n",
    "plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations above 0.5 will be considered noteworthy. In this correlation matrix, we see the following strong correlations:\n",
    "\n",
    "**PercentTradesNeverDelq - ExternalRiskEstimate: 0.56**\n",
    "\n",
    "**ExternalRiskEstimate - MSinceMostRecentDelq: 0.63**\n",
    "\n",
    "**PercentTradesNeverDelq - MSinceMostRecentDelq: 0.64**\n",
    "\n",
    "**NumTotalTrades - NumSatisfactoryTrades: 0.93**\n",
    "\n",
    "**NetFractionRevolvingBurden - NumBank2NatlTradesWHighUtilization: 0.58**\n",
    "\n",
    "**NumRevolvingTradesWBalance - NumBank2NatlTradesWHighUtilization: 0.62**\n",
    "\n",
    "**PercentTradesWBalance - NetFractionRevolvingBurden: 0.570**\n",
    "\n",
    "**AverageMInFile - MSinceOldestTradeOpen: 0.74**\n",
    "\n",
    "**NumSatisfactoryTrades - NumTradesWBalance: 0.7**\n",
    "\n",
    "**NumTradesWBalance - NumTotalTrades: 0.74**\n",
    "\n",
    "**NumTradesWBalance - NumRevolvingTradesWBalance: 0.61** \n",
    "\n",
    "**PercentTradesNeverDelq - ExternalRiskEstimate**\n",
    "There is a slightly strong correlation between these two features, and as stated in the supplied \"data dictionary\" CSV, ExternalRiskEstimate is \"Monotonically Decreasing\", meaning that an increase in ExternalRiskEstimate leads to a decrease in the probability of a \"Bad\" target outcome. This relationship makes sense, as past performance on delinquency intuitively leads to predictions on future performance.   \n",
    "\n",
    "**ExternalRiskEstimate - MSinceMostRecentDelq**\n",
    "There is a reasonably high correlation present here, which makes sense as the longer it has been since an entry has had a delinquency associated with it, the less risky it will be when it comes to repayments.\n",
    "\n",
    "**PercentTradesNeverDelq - MSinceMostRecentDelq**:\n",
    "There is a reasonably high correlation present here, which indicates that the entries which have not had a deliquency in a long time (or ever), have a higher percentage of trades that were never delinquent. This makes sense because the longer it has been since a delinquency, the longer the amount of time satisfactory trades have been occuring.\n",
    "\n",
    "**NumTotalTrades - NumSatisfactoryTrades**\n",
    "There is a very high correlation here, almost 1:1. This points out that the vast majority of all trades are satisfactory. \n",
    "\n",
    "**NetFractionRevolvingBurden - NumBank2NatlTradesWHighUtilization**\n",
    "There is a reasonably high correlation here. NumBank2NatlTradesWHighUtilization \"counts the number of credit cards on a consumer credit bureau report carrying a balance that is at 75% of its limit or greater.\" Therefore, it can be seen that customers with a high fraction of burden on revolving type accounts are quite likely to have high utilization (75%+). \n",
    "\n",
    "**NumRevolvingTradesWBalance - NumBank2NatlTradesWHighUtilization**\n",
    "Following on from the above point, it can be noted that quite a high percentage of people with revolving accounts that have balance, have a balance that is 75% of its limit or greater. This would tend to suggest that customers with balance on revolving accounts are likely to at risk of missing repayments on time. This correlation will be examined further in the target feature - continuous feature correlations in the next segment.\n",
    "\n",
    "**PercentTradesWBalance - NetFractionRevolvingBurden**\n",
    "Revolving accounts appear again, with this correlation showing that the majority of trades which have balance are in fact revolving accounts. It would seem that, in general, revolving accounts have more balance to be repaid than installment accounts.\n",
    "\n",
    "**AverageMInFile - MSinceOldestTradeOpen**\n",
    "There is quite a strong correlation here, which makes sense as the longer a customer is with the company, the more opportunities to be in file exist. \n",
    "\n",
    "**NumSatisfactoryTrades - NumTradesWBalance**\n",
    "There is quite a strong relationship here, which makes sense as it has already been shown that NumTotalTrades has an extremely high correlation with NumSatisfactoryTrades, and NumTradesWBalance is a subset of NumTotalTrades.\n",
    "\n",
    "**NumTradesWBalance - NumTotalTrades**\n",
    "This high correlation indicates that the majority of trades have remaining balance, which shows that the company has more ongoing trades than completed trades. \n",
    "\n",
    "**NumTradesWBalance - NumRevolvingTradesWBalance** \n",
    "This reasonably strong correlation indicates that the majority of trades with balance are revolving trades, and as discussed earlier, the majority of these have high utilization. Therefore, the company has a large about of high utilization trades. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots\n",
    "\n",
    "A better visualisation of the continuous features can seen through the use of scatter plots. If any two continuous features have a very high correlation (0.9+), then only one will be chosen for the predictive models if both have a high correlation with the target feature, as having two very highly correlated features in the predictive model can lead to issues (as one of the very highly correlated continuous features offers little extra information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatterplots for each descriptive feature and target feature. \n",
    "# Show the correlation value in the plot.\n",
    "# This allows us to check strength of correlation with the target feature.\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharey=False)\n",
    "traindf.plot(kind='scatter', x='PercentTradesNeverDelq', y='ExternalRiskEstimate', label=\"%.3f\" % df[['PercentTradesNeverDelq', 'ExternalRiskEstimate']].corr().as_matrix()[0,1], ax=axs[0], figsize=(20, 5))\n",
    "traindf.plot(kind='scatter', x='ExternalRiskEstimate', y='MSinceMostRecentDelq', label=\"%.3f\" % df[['ExternalRiskEstimate', 'MSinceMostRecentDelq']].corr().as_matrix()[0,1], ax=axs[1])\n",
    "traindf.plot(kind='scatter', x='PercentTradesNeverDelq', y='MSinceMostRecentDelq', label=\"%.3f\" % df[['PercentTradesNeverDelq', 'MSinceMostRecentDelq']].corr().as_matrix()[0,1], ax=axs[2])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharey=False)\n",
    "traindf.plot(kind='scatter', x='NumTotalTrades', y='NumSatisfactoryTrades', label=\"%.3f\" % df[['NumTotalTrades', 'NumSatisfactoryTrades']].corr().as_matrix()[0,1], ax=axs[0], figsize=(20, 5))\n",
    "traindf.plot(kind='scatter', x='NetFractionRevolvingBurden', y='NumBank2NatlTradesWHighUtilization', label=\"%.3f\" % df[['NetFractionRevolvingBurden', 'NumBank2NatlTradesWHighUtilization']].corr().as_matrix()[0,1], ax=axs[1])\n",
    "traindf.plot(kind='scatter', x='NumRevolvingTradesWBalance', y='NumBank2NatlTradesWHighUtilization', label=\"%.3f\" % df[['NumRevolvingTradesWBalance', 'NumBank2NatlTradesWHighUtilization']].corr().as_matrix()[0,1], ax=axs[2])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharey=False)\n",
    "traindf.plot(kind='scatter', x='PercentTradesWBalance', y='NetFractionRevolvingBurden', label=\"%.3f\" % df[['PercentTradesWBalance', 'NetFractionRevolvingBurden']].corr().as_matrix()[0,1], ax=axs[0], figsize=(20, 5))\n",
    "traindf.plot(kind='scatter', x='AverageMInFile', y='MSinceOldestTradeOpen', label=\"%.3f\" % df[['AverageMInFile', 'MSinceOldestTradeOpen']].corr().as_matrix()[0,1], ax=axs[1])\n",
    "traindf.plot(kind='scatter', x='NumSatisfactoryTrades', y='NumTradesWBalance', label=\"%.3f\" % df[['NumSatisfactoryTrades', 'NumTradesWBalance']].corr().as_matrix()[0,1], ax=axs[2])\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=False)\n",
    "traindf.plot(kind='scatter', x='NumTradesWBalance', y='NumTotalTrades', label=\"%.3f\" % df[['NumTradesWBalance', 'NumTotalTrades']].corr().as_matrix()[0,1], ax=axs[0], figsize=(20, 5))\n",
    "traindf.plot(kind='scatter', x='NumTradesWBalance', y='NumRevolvingTradesWBalance', label=\"%.3f\" % df[['NumTradesWBalance', 'NumRevolvingTradesWBalance']].corr().as_matrix()[0,1], ax=axs[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It can be seen that NumTotalTrades and NumSatisfactoryTrades are very highly correlated. Thus, if they are seen to have a high correlation with the target feature in the tests below, a decision will have to be made about which of the features will be used in the predictive model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlation between the continuous features in the dataframe and the target features are set out below, with the goal of finding the most promising features to use in the predictive models. A promising feature is one which shows a high correlation with the target outcome. A higher correlation between features leads to more accurate predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "flierprops = dict(marker='o', markerfacecolor='green', markersize=6,\n",
    "                  linestyle='none')\n",
    "\n",
    "traindf.boxplot(column=['ExternalRiskEstimate'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['MSinceOldestTradeOpen'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['AverageMInFile'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "traindf.boxplot(column=['NumSatisfactoryTrades'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['PercentTradesNeverDelq'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['NumRevolvingTradesWBalance'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "\n",
    "traindf.boxplot(column=['NumBank2NatlTradesWHighUtilization'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['PercentTradesWBalance'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['MSinceMostRecentDelq'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "traindf.boxplot(column=['NumTotalTrades'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['NetFractionInstallBurden'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['NetFractionRevolvingBurden'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "traindf.boxplot(column=['NumTrades90Ever2DerogPubRec'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "\n",
    "traindf.boxplot(column=['NumTradesOpeninLast12M'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['MSinceMostRecentInqexcl7days'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['MSinceMostRecentTradeOpen'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "traindf.boxplot(column=['NumInstallTradesWBalance'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['PercentInstallTrades'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['NumInqLast6Mexcl7days'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "\n",
    "traindf.boxplot(column=['PercentSatisfactoryTrades'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    "traindf.boxplot(column=['NumTradesWBalance'], by=['RiskPerformance'], flierprops=flierprops, figsize=(10,7))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From analysing these plots, the following information has been deemed noteworthy:\n",
    "\n",
    "**ExternalRiskEstimate - RiskPerformance**\n",
    "From the plot it can be seen that a higher ExternalRiskEstimate leads to a lower chance of receiving a 'Bad' target outcome. There is a strong relationship displayed on the plot, and thus this is a prime candidate for use in the predictive data model. The work of external risk estimators leads to a more confident decision in whether the customer is perceived to be able to make their repayments on time.\n",
    "\n",
    "**AverageMInFile - RiskPerformance**\n",
    "While the relationship is not as strong as that seen with 'ExternalRiskEstimate', there is still a correlation between 'AverageMInFile' and the target feature. As 'AverageMInFile' increases, so does the probability of a 'Good' target outcome. This makes intuitive sense, as longer term customers have more data associated with them which can be used to determine their probability of repayment.\n",
    "\n",
    "**NumBank2NatlTradesWHighUtilization**\n",
    "Even with an abundance of outliers (which were not clamped due to the decision that this data was highly probable to be correct), there is a link between 'NumBank2NatlTradesWHighUtilization' and the target feature, with a higher utilization pointing to a higher chance of a 'Bad' target outcome. This makes intuitive sense, as a customer with 75%+ utilization has a large amount of money to repay and thus may struggle to do so in a timely manner.\n",
    "\n",
    "**NumSatisfactoryTrades**\n",
    "Interestingly, the number of satisfactory trades doesn't seem to have a strong correlation with the target feature, which is counterintuitive. This points to other factors being weighted more heavily in the decision. This is therefore not a good candidate for the predictive model.\n",
    "\n",
    "Also, as NumTotalTrades has a high correlation with NumSatisfactoryTrades, we can see that this feature also doesn't have a high correlation with the target feature. Thus, neither will be used in the predictive model.\n",
    "\n",
    "**PercentTradesWBalance**\n",
    "Upon inspecting this plot, it can be seen that there is a correlation with the target feature. While customers with a 0 to 100% balance can still achieve 'Good' outcomes, the average is still weighted more heavily towards a 'Bad' outcome. With an increase in trades with balance, it can generally be seen that there is an increase in the chance that a 'Bad' target outcome will occur. This makes sense as a higher amount to repay intuitively means there is a higher chance that an on time repayment will be less likely.  \n",
    "\n",
    "**MSinceMostRecentDelq**\n",
    "Here it can be seen that the longer it was since a delinquency, the higher the chance of a 'Good' target outcome. This makes sense, as a customer who recently missed repayments is more likely to do so in the near future. \n",
    "\n",
    "\n",
    "**NetFractionRevolvingBurden**\n",
    "There is quite a strong correlation here, with a higher fraction of burden on a revolving account meaning a higher chance of a 'Bad' outcome. A more burdened account will be more difficult to repay, and thus the risk increases. \n",
    "\n",
    "\n",
    "**MSinceOldestTradeOpen**\n",
    "There is a reasonably strong correlation with the target feature here. Perhaps having a record on the books for a longer period of time allows the company to have a better idea on the repayment risks of their customers, as we see that the greater the number of months since the oldest trade was open, the higher the chance of receiving a \"Good\" target outcome. \n",
    "\n",
    "\n",
    "**Decision on subset to choose for the predicitive model:**\n",
    "\n",
    "**The decision has been made to choose:**\n",
    "- ExternalRiskEstimate\n",
    "- AverageMInFile\n",
    "- PercentTradesWBalance\n",
    "- MSinceMostRecentDelq\n",
    "- NetFractionRevolvingBurden\n",
    "- NumBank2NatlTradesWHighUtilization\n",
    "- MSinceOldestTradeOpen\n",
    "\n",
    "This decision was made due to the overall strong correlations of these features with the target feature. The other features in the plot have lesser degrees of correlation, and thus would not add much usefulness in the creation of predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlation between the categorical features in the dataframe and the target features are set out below, with the goal of finding the most promising features to use in the predictive models. A promising feature is one which shows a high correlation with the target outcome. A higher correlation between features leads to more accurate predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxDelq2PublicRecLast12M = pd.unique(traindf.MaxDelq2PublicRecLast12M.ravel())\n",
    "dfnew = traindf.copy()\n",
    "dfnew['percent'] = 0\n",
    "\n",
    "for i in MaxDelq2PublicRecLast12M:\n",
    "    count = 1 / dfnew[dfnew.MaxDelq2PublicRecLast12M == i].count()['RiskPerformance']\n",
    "    index_list = dfnew[dfnew['MaxDelq2PublicRecLast12M'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        dfnew.loc[ind, 'percent'] = count * 100\n",
    "        \n",
    "group = dfnew[['percent','MaxDelq2PublicRecLast12M','RiskPerformance']].groupby(['MaxDelq2PublicRecLast12M','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"RiskPerformance based on MaxDelq2PublicRecLast12M\", figsize=(15,7))\n",
    "\n",
    "red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "my_plot.set_xlabel(\"MaxDelq2PublicRecLast12M\")\n",
    "my_plot.set_ylabel(\"RiskPerformance\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxDelqEver = pd.unique(traindf.MaxDelqEver.ravel())\n",
    "dfnew = traindf.copy()\n",
    "dfnew['percent'] = 0\n",
    "\n",
    "for i in MaxDelqEver:\n",
    "    count = 1 / dfnew[dfnew.MaxDelqEver == i].count()['RiskPerformance']\n",
    "    index_list = dfnew[dfnew['MaxDelqEver'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        dfnew.loc[ind, 'percent'] = count * 100\n",
    "        \n",
    "group = dfnew[['percent','MaxDelqEver','RiskPerformance']].groupby(['MaxDelqEver','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"RiskPerformance based on MaxDelqEver\", figsize=(15,7))\n",
    "\n",
    "red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "my_plot.set_xlabel(\"MaxDelqEver\")\n",
    "my_plot.set_ylabel(\"RiskPerformance\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DelqEver = pd.unique(traindf.DelqEver.ravel())\n",
    "dfnew = traindf.copy()\n",
    "dfnew['percent'] = 0\n",
    "\n",
    "for i in DelqEver:\n",
    "    count = 1 / dfnew[dfnew.DelqEver == i].count()['RiskPerformance']\n",
    "    index_list = dfnew[dfnew['DelqEver'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        dfnew.loc[ind, 'percent'] = count * 100\n",
    "        \n",
    "group = dfnew[['percent','DelqEver','RiskPerformance']].groupby(['DelqEver','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"RiskPerformance based on DelqEver\", figsize=(15,7))\n",
    "\n",
    "red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "my_plot.set_xlabel(\"DelqEver\")\n",
    "my_plot.set_ylabel(\"RiskPerformance\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DelqLast12M = pd.unique(traindf.DelqLast12M.ravel())\n",
    "dfnew = traindf.copy()\n",
    "dfnew['percent'] = 0\n",
    "\n",
    "for i in DelqLast12M:\n",
    "    count = 1 / dfnew[dfnew.DelqLast12M == i].count()['RiskPerformance']\n",
    "    index_list = dfnew[dfnew['DelqLast12M'] == i].index.tolist()\n",
    "    for ind in index_list:\n",
    "        dfnew.loc[ind, 'percent'] = count * 100\n",
    "        \n",
    "group = dfnew[['percent','DelqLast12M','RiskPerformance']].groupby(['DelqLast12M','RiskPerformance']).sum()\n",
    "\n",
    "my_plot = group.unstack().plot(kind='bar', stacked=True, title=\"RiskPerformance based on DelqLast12M\", figsize=(15,7))\n",
    "\n",
    "red_patch = mpatches.Patch(color='orange', label='Good')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Bad')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "my_plot.set_xlabel(\"DelqLast12M\")\n",
    "my_plot.set_ylabel(\"RiskPerformance\")\n",
    "my_plot.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MaxDelq2PublicRecLast12M**\n",
    "The following special meanings are attached to these numbers:\n",
    "\n",
    "- 0\tderogatory comment\n",
    "- 1\t120+ days delinquent\n",
    "- 2\t90 days delinquent\n",
    "- 3\t60 days delinquent\n",
    "- 4\t30 days delinquent\n",
    "- 5, 6\tunknown delinquency\n",
    "- 7\tcurrent and never delinquent\n",
    "- 8, 9\tall other\n",
    "\n",
    "Customers with the value 7 have the highest amount of 'Good' outcomes compared to the other values. This makes sense, as 7 means current and never delinquent, and thus the customer has a good record for repayment. The other values have a higher correlation with 'Bad' target outcomes, which is to be expected as either the customer has a history of delinquency, or the delinquency is unknown.\n",
    "\n",
    "**MaxDelqEver**\n",
    "The following special meanings are attached to these numbers: \n",
    "\n",
    "- 1\tNo such value\n",
    "- 2\tderogatory comment\n",
    "- 3\t120+ days delinquent\n",
    "- 4\t90 days delinquent\n",
    "- 5\t60 days delinquent\n",
    "- 6\t30 days delinquent\n",
    "- 7\tunknown delinquency\n",
    "- 8\tcurrent and never delinquent\n",
    "- 9\tall other\n",
    "\n",
    "This is a very similar story to MaxDelq2PublicRecLast12M above. Customers with the value 8 have the highest amount of 'Good' outcomes compared to the other values. This makes sense, as 8 means current and never delinquent, and thus the customer has a good record for repayment. The other values have a higher correlation with 'Bad' target outcomes, which is to be expected as either the customer has a history of delinquency, or the delinquency is unknown.\n",
    "\n",
    "**DelqEver** \n",
    "\n",
    "DelqEver measures if a entry during their history has ever been delinquent, and it can be seen from the chart that have a deliquency on a record points to a greater chance of a \"Bad\" target outcome. This makes sense, as a previous failure to repay on time intuitively points to a greater risk of it happening again.\n",
    "\n",
    "**DelqLast12M**\n",
    "\n",
    "DelqEver measures if a entry during has been delinquent in the last 12 months, and it can be seen from the chart that have a deliquency in the last 12 months on a record points to a high chance of a \"Bad\" target outcome. This makes sense, as a previous failure to repay on time, especially recently, intuitively points to a greater risk of it happening again.\n",
    "\n",
    "**Decision on subset to choose for the predicitive model:**\n",
    "\n",
    "**The decision has been made to choose:**\n",
    "MaxDelq2PublicRecLast12M, MaxDelqEver, DelqEver, and DelqLast12M were chosen, as the values which a customer receives for these features has been seen to have a large effect on the target outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final chosen subset:\n",
    "**Continuous:** \n",
    "- ExternalRiskEstimate \n",
    "- AverageMInFile \n",
    "- PercentTradesWBalance\n",
    "- MSinceMostRecentDelq \n",
    "- NetFractionRevolvingBurden \n",
    "- MSinceOldestTradeOpen\n",
    "- NumBank2NatlTradesWHighUtilization\n",
    "\n",
    "\n",
    "**Categorical:** \n",
    "- MaxDelq2PublicRecLast12M\n",
    "- MaxDelqEver\n",
    "- DelqEver\n",
    "- DelqLast12M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The subset of categorical and continuous features have now been chosen, and thus the creation and evaluation of the predictive models may now begin. First, the data shall be prepared for these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The predictive models need numerical data to function properly, and thus, categorical data must be changed to numerical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy values will be used to give categorical data numerical values, which are needed for the predictive model descriptors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummy values for the categorical subset chosen. \n",
    "\n",
    "RiskPerformance_dummies = pd.get_dummies(traindf['RiskPerformance'], prefix='RiskPerformance', drop_first=True)\n",
    "print(\"RiskPerformance:\", RiskPerformance_dummies)\n",
    "\n",
    "MaxDelqEver_dummies = pd.get_dummies(traindf['MaxDelqEver'], prefix='MaxDelqEver', drop_first=True)\n",
    "print(\"MaxDelqEver:\", MaxDelqEver_dummies)\n",
    "\n",
    "MaxDelq2PublicRecLast12M_dummies = pd.get_dummies(traindf['MaxDelq2PublicRecLast12M'], prefix='MaxDelq2PublicRecLast12M', drop_first=True)\n",
    "print(\"MaxDelq2PublicRecLast12M:\", MaxDelq2PublicRecLast12M_dummies)\n",
    "\n",
    "DelqEver_dummies = pd.get_dummies(traindf['DelqEver'], prefix='DelqEver', drop_first=True)\n",
    "print(\"DelqEver:\", DelqEver_dummies)\n",
    "\n",
    "DelqLast12M_dummies = pd.get_dummies(traindf['DelqLast12M'], prefix='DelqLast12M', drop_first=True)\n",
    "print(\"DelqLast12M:\", DelqLast12M_dummies)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model will be trained, or 'fit', using all of the chosen subset of categorical and continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "cont_features = ['ExternalRiskEstimate', 'PercentTradesWBalance', 'NetFractionRevolvingBurden',\n",
    "                'AverageMInFile', 'MSinceMostRecentDelq', 'MSinceOldestTradeOpen', 'NumBank2NatlTradesWHighUtilization']\n",
    "\n",
    "\n",
    "# Continuous and categorical features combined \n",
    "features = cont_features + RiskPerformance_dummies.columns.values.tolist() + MaxDelqEver_dummies.columns.values.tolist() + MaxDelq2PublicRecLast12M_dummies.columns.values.tolist() + DelqEver_dummies.columns.values.tolist() + DelqLast12M_dummies.columns.values.tolist()\n",
    "\n",
    "print(\"Features: \", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new dataframe will be created which will have the dummy values added, and the catergory data type features removed. Only the chosen subset will be in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_chosFeat = pd.concat([traindf, RiskPerformance_dummies, MaxDelqEver_dummies, MaxDelq2PublicRecLast12M_dummies, DelqEver_dummies, DelqLast12M_dummies], axis=1)\n",
    "# Keep the features chosen.\n",
    "feat_to_keep = features\n",
    "\n",
    "traindf_chosFeat = traindf_chosFeat.loc[:, feat_to_keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the head of the new df to ensure it contains the wanted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_chosFeat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first predictive model will now be created and evaluated. This model is the Multiple Linear Regression model. This model works by fitting a linear equation to observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the descriptive features, which will be used to try predict the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decriptive features, X_train, for the model will be all chosen features except for the target, which will be y_train.\n",
    "\n",
    "X_train = traindf_chosFeat[[x for x in traindf_chosFeat[features] if x not in ['RiskPerformance_Good']]]\n",
    "y_train = traindf_chosFeat.RiskPerformance_Good\n",
    "\n",
    "print(\"\\nDescriptive features in X:\\n\", X_train)\n",
    "print(\"\\nTarget feature in y:\\n\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_chosFeat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train aka fit a model using all chosen continuous and categorical features on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiple_linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Print the intercept\n",
    "print(\"\\nIntercept: \\n\", multiple_linreg.intercept_)\n",
    "print()\n",
    "# Print the features and coefficients\n",
    "print(\"Features and coefficients:\", list(zip(X_train, multiple_linreg.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear regression model works based on estimating a set of weights per feature, and also an extra weight called the intercept. The intercept is the mean of Y (the target) when all predictors equal zero. It is an adjustment parameter called the \"bias\", and gives the base value of the target.\n",
    "\n",
    "$target\\_feature = w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n $\n",
    "\n",
    "From the results of this model, it can be seen that the base value for RiskPerformance is -0.8727520494674129.\n",
    "\n",
    "The weights for the other features are as follows:\n",
    "\n",
    "- For every unit increase in **ExternalRiskEstimate** the RiskPerformance target increases by: **0.014011095261030292**\n",
    "- For every unit increase in **PercentTradesWBalance** the RiskPerformance target increases by: **0.001829684339055234**\n",
    "- For every unit increase in **NetFractionRevolvingBurden** the RiskPerformance target decreases by: **-0.0031226344582442663**\n",
    "- For every unit increase in **AverageMInFile** the RiskPerformance target increases by: **0.0026798888878722844**\n",
    "- For every unit increase in **MSinceMostRecentDelq** the RiskPerformance target decreases by: **-0.0009084575942394557**\n",
    "- For every unit increase in **MSinceOldestTradeOpen** the RiskPerformance target increases by: **0.0004927879034892035**\n",
    "- For every unit increase in **NumBank2NatlTradesWHighUtilization** the RiskPerformance target decreases by: **-0.012732620803912575**\n",
    "- For every unit increase in **MaxDelqEver_3** the RiskPerformance target decreases by: **-0.16715543030913155**\n",
    "- For every unit increase in **MaxDelqEver_4** the RiskPerformance target decreases by: **-0.002765512726690825**\n",
    "- For every unit increase in **MaxDelqEver_5** the RiskPerformance target decreases by: **-0.019637458917251053**\n",
    "- For every unit increase in **MaxDelqEver_6** the RiskPerformance target increases by: **0.1124144744351723**\n",
    "- For every unit increase in **MaxDelqEver_7** the RiskPerformance target increases by: **0.06548184122182071**\n",
    "- For every unit increase in **MaxDelq2PublicRecLast12M_3** the RiskPerformance target increases by: **0.17336707488613431**\n",
    "- For every unit increase in **MaxDelq2PublicRecLast12M_4** the RiskPerformance target decreases by: **-0.1089470561126091**\n",
    "- For every unit increase in **MaxDelq2PublicRecLast12M_6** the RiskPerformance target increases by: **0.11944537854390623**\n",
    "- For every unit increase in **MaxDelq2PublicRecLast12M_7** the RiskPerformance target increases by: **0.10635333789162385**\n",
    "- For every unit increase in **DelqEver_True** the RiskPerformance target decreases by: **-0.06548184122182067**\n",
    "- For every unit increase in **DelqLast12M_True** the RiskPerformance target decreases by: **-0.10635333789162374**\n",
    "\n",
    "### Thus, the model is:\n",
    "\n",
    "**RiskPerformance_Good** = -0.8727520494674129 + ExternalRiskEstimate * 0.014011095261030292 + PercentTradesWBalance * 0.001829684339055234 - NetFractionRevolvingBurden * 0.0031226344582442663 + AverageMInFile * 0.0026798888878722844 - MSinceMostRecentDelq * 0.0009084575942394557 + MSinceOldestTradeOpen * 0.0004927879034892035 - NumBank2NatlTradesWHighUtilization * 0.012732620803912575 - MaxDelqEver_3 * 0.16715543030913155 - MaxDelqEver_4 * 0.002765512726690825 - MaxDelqEver_5 * 0.019637458917251053 + MaxDelqEver_6* 0.1124144744351723 + MaxDelqEver_7 * 0.06548184122182071 + MaxDelq2PublicRecLast12M_3 * 0.17336707488613431 - MaxDelq2PublicRecLast12M_4 * 0.1089470561126091 + MaxDelq2PublicRecLast12M_6 * 0.11944537854390623 + MaxDelq2PublicRecLast12M_7 * 0.10635333789162385 - DelqEver_True * 0.06548184122182067 - DelqLast12M_True * 0.10635333789162374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the model has been trained, it will be tested on the training data. The first 100 training examples will be printed to ensure that the model is working correctly (debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict on the training descriptive features.\n",
    "multiple_linreg_predictions = multiple_linreg.predict(X_train)\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "\n",
    "# Realign indices\n",
    "y_train.reset_index(drop=True)\n",
    "noThresPredDf = pd.DataFrame(multiple_linreg_predictions, columns=['Predicted'])\n",
    "noThresPredDf.reset_index(drop=True)\n",
    "\n",
    "# Show the actual vs predicted values for the first 100 training examples.\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train.head(100), noThresPredDf.head(100)], axis=1)\n",
    "\n",
    "print(actual_vs_predicted_multiplelinreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The predicted values are in a range from 0 to 1, but for the target in this data, the outcome can only be 0 or 1, nothing in between. Thus, the predicted results will be set to 0 or 1. The threshold is set as 0.5, with 0 if the prediction is below 0.5, and 1 is the prediction is 0.5 or greater. This gives a class to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set less than 0.5 to 0, and 0.5 and greater to 1.\n",
    "preddf = pd.DataFrame(multiple_linreg_predictions, columns=['Predicted'])\n",
    "preddf.loc[preddf.Predicted <= 0.5, 'predicted_Yes_No'] = 0 \n",
    "preddf.loc[preddf.Predicted > 0.5, 'predicted_Yes_No'] = 1 \n",
    "preddf = preddf.drop(\"Predicted\", axis=1)\n",
    "\n",
    "# Reset the indices to allow the rows to align correctly.\n",
    "y_train.reset_index(drop=True)\n",
    "preddf.reset_index(drop=True)\n",
    "\n",
    "# Show the actual vs predicted values for the first 100 training examples.\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train.head(100), preddf.head(100)], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A comparision can now be made more clearly, with the actual and predicted both being either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printMetrics function has been created to give evalution metrics throughout this work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used repeatedly to compute all metrics\n",
    "def printMetrics(testActualVal, predictions):\n",
    "    #classification evaluation measures\n",
    "    print('\\n==============================================================================')\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(testActualVal, predictions))\n",
    "    print(\"MAE: \", metrics.mean_absolute_error(testActualVal, predictions))\n",
    "    print(\"MSE: \", metrics.mean_squared_error(testActualVal, predictions))\n",
    "    print(\"RMSE: \", metrics.mean_squared_error(testActualVal, predictions)**0.5)\n",
    "    print(\"R2: \", metrics.r2_score(testActualVal, predictions))\n",
    "    print('\\n==============================================================================')\n",
    "    print(\"Confusion matrix: \\n\", metrics.confusion_matrix(testActualVal, predictions))\n",
    "    print(\"Classification report:\\n \", metrics.classification_report(testActualVal, predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train, preddf['predicted_Yes_No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the metrics:\n",
    "\n",
    "The accuracy is 74.3%, which means that the model predicts correctly 74.3% of the time, which is better than a 50/50 choice but still not very accurate.\n",
    "\n",
    "The confusion matrix reveals 235 true negatives, 82 false negatives, 69 false positives, and 202 true positives.\n",
    "\n",
    "The weighted average precision, (correctly predicted positive / predicted positive) is 74%, which shows that when the model predicts a good target outcome, it is correct 74% of the time. Precision is the ratio of correctly predicted positive (here it is a 'good' outcome) observations to the total predicted positive observations.\n",
    "\n",
    "The weighted average recall, (correctly predicted positive / actual positive) is 74%, which shows when the target outcome is good, it predicts that it is good 74% of the time. (When it's actually good, it predicts good 74% of the time.)\n",
    "\n",
    "The weighted average F1-score, (aggregation of Precision and Recall) is 74%, which means there are a reasonably low amount of false positives and false negatives.\n",
    "\n",
    "The MAE and MSE are both 0.2568027210884354, the RMSE is 0.5067570631855419, and the R2 is -0.02840066716085965.\n",
    "\n",
    "In general, the higher the R2, the better the model fits the data, and the lower the MSE, MAE, and RMSE, the lower the error in the model. In this case, a minus value for R2 could indicate a poor fit for the model with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation of the model with the test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the dummies and features dataframe setup for the test dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RiskPerformance_dummies = pd.get_dummies(testdf['RiskPerformance'], prefix='RiskPerformance', drop_first=True)\n",
    "print(\"RiskPerformance:\", RiskPerformance_dummies)\n",
    "\n",
    "MaxDelqEver_dummies = pd.get_dummies(testdf['MaxDelqEver'], prefix='MaxDelqEver', drop_first=True)\n",
    "print(\"MaxDelqEver:\", MaxDelqEver_dummies)\n",
    "\n",
    "MaxDelq2PublicRecLast12M_dummies = pd.get_dummies(testdf['MaxDelq2PublicRecLast12M'], prefix='MaxDelq2PublicRecLast12M', drop_first=True)\n",
    "print(\"MaxDelq2PublicRecLast12M:\", MaxDelq2PublicRecLast12M_dummies)\n",
    "\n",
    "DelqEver_dummies = pd.get_dummies(testdf['DelqEver'], prefix='DelqEver', drop_first=True)\n",
    "print(\"DelqEver:\", DelqEver_dummies)\n",
    "\n",
    "DelqLast12M_dummies = pd.get_dummies(testdf['DelqLast12M'], prefix='DelqLast12M', drop_first=True)\n",
    "print(\"DelqLast12M:\", DelqLast12M_dummies)\n",
    "\n",
    "\n",
    "cont_features = ['ExternalRiskEstimate', 'PercentTradesWBalance', 'NetFractionRevolvingBurden',\n",
    "                'AverageMInFile', 'MSinceMostRecentDelq', 'MSinceOldestTradeOpen', 'NumBank2NatlTradesWHighUtilization']\n",
    "\n",
    "features = cont_features + RiskPerformance_dummies.columns.values.tolist() + MaxDelqEver_dummies.columns.values.tolist() + MaxDelq2PublicRecLast12M_dummies.columns.values.tolist() + DelqEver_dummies.columns.values.tolist() + DelqLast12M_dummies.columns.values.tolist()\n",
    "print(\"\\nCont features: \", cont_features)\n",
    "# print(\"Train Categ features: \", train_categ_features)\n",
    "print(\"Features: \", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_chosFeat = pd.concat([testdf, RiskPerformance_dummies, MaxDelqEver_dummies, MaxDelq2PublicRecLast12M_dummies, DelqEver_dummies, DelqLast12M_dummies], axis=1)\n",
    "\n",
    "feat_to_keep = features\n",
    "\n",
    "testdf_chosFeat = testdf_chosFeat.loc[:, feat_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_chosFeat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide the test dataframe into X_test with the descriptive features, and y_test with the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testdf_chosFeat[[x for x in testdf_chosFeat[features] if x not in ['RiskPerformance_Good']]]\n",
    "y_test = testdf_chosFeat.RiskPerformance_Good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the predictive model on the test dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predictions = multiple_linreg.predict(X_test)\n",
    "print(\"Actual values of test:\\n\", y_test)\n",
    "print(\"Predictions on test:\", test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preddf = pd.DataFrame(test_predictions, columns=['Predicted'])\n",
    "test_preddf.loc[test_preddf.Predicted <= 0.5, 'predicted_Yes_No'] = 0 \n",
    "test_preddf.loc[test_preddf.Predicted > 0.5, 'predicted_Yes_No'] = 1 \n",
    "test_preddf = test_preddf.drop(\"Predicted\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test, test_preddf['predicted_Yes_No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the metrics:\n",
    "\n",
    "The accuracy is 70.6%, which means that the model predicts correctly 70.6% of the time, which is better than a 50/50 choice but still not very accurate. The model is 3.7% less accurate than on the training data.\n",
    "\n",
    "The confusion matrix reveals 91 true negatives, 33 false negatives, 41 false positives, and 87 true positives.\n",
    "\n",
    "The weighted average precision, (correctly predicted positive / predicted positive) is 71%, which shows that when the model predicts a good target outcome, it is correct 71% of the time. Precision is the ratio of correctly predicted positive (here it is a 'good' outcome) observations to the total predicted positive observations.\n",
    "\n",
    "The weighted average recall, (correctly predicted positive / actual positive) is 71%, which shows when the target outcome is good, it predicts that it is good 74% of the time. (When it's actually good, it predicts good 71% of the time.)\n",
    "\n",
    "The weighted average F1-score, (aggregation of Precision and Recall) is 71%, which means there are a reasonably low amount of false positives and false negatives.\n",
    "\n",
    "The scores all are lower on the test data than the training data. This is to be expected, as the test data is new data for the model, with the model having the chance to \"learn\" the results of some of the training data. \n",
    "\n",
    "The MAE, MSE and RMSE are all higher, indicating more error in the model when used with the test data. The R2 is also lower than before. This points to the model performing worse on the test data than on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_val_score() is normally used to generate scores through cross-validation, however in this dataset the target needs to have a threshold, and thus cross_val_score() won't return accurate results. Instead, it has been decided that cross_val_predict() will we used to generate cross-validiated predictions, which will then be thresholded, and metrics applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_val_predict() returns cross-validated prediction estimates for each element in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 new dataframes, one with all of x, one with all of y. This represents the entirety of the data.\n",
    "crossValXdf=pd.concat([X_train, X_test])\n",
    "crossValYdf=pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crossValXdf= crossValXdf.reset_index(drop=True)\n",
    "crossValXdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(LinearRegression(), crossValXdf, crossValYdf, cv=3)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply thresholding\n",
    "for i, predict in enumerate(predictions):\n",
    "     predictions[i] = 0 if (predict <= 0.5) else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics of the cross-validated predictions\n",
    "printMetrics(predictions, crossValYdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation vs Test data results\n",
    "\n",
    "Analysis of the metrics:\n",
    "\n",
    "The accuracy is 70.6% for the test data, and 72.26% for the cross validation data.\n",
    "\n",
    "The weighted average precision, recall, and F1-score for the test data is 71%, and 72% for the cross validation data.\n",
    "\n",
    "The cross validated model has lower MAE, MSE, and RMSE scores, indicating less error, and the R2 is higher which indicates a better fit for the model.\n",
    "\n",
    "The MAE, MSE and RMSE are all higher, indicating more errors in the model when used with the test data. The R2 is also lower than before. This points to the model performing worse on the test data than on the training data.\n",
    "\n",
    "Therefore, the cross validated model is a better predictive model than that used on the test data. This is largely down to the cross validated model having access to a larger amount of data sets.\n",
    "\n",
    "### Linear regression evaluation\n",
    "\n",
    "The accuracy scores of 70.6% for the test model and 72.26% for the cross validation model are better than 50/50 chance, but it is still not highly accurate. The low r2 scores seem to indicate that this model is not a great fit for the data. Therefore, further models will be explored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression is the next predictive model which will be evaluated. This models the probabilities for classification problems which have two possible outcomes. In classification we can interpret the target feature as the probability of class membership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model using the chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous features.\n",
    "multiple_logreg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Print the weights learned for each feature.\n",
    "print(\"Features: \\n\", cont_features)\n",
    "print(\"Coefficients: \\n\", multiple_logreg.coef_)\n",
    "print(\"\\nIntercept: \\n\", multiple_logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$probability(target=1|descriptive\\_features)=w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n$\n",
    "\n",
    "\n",
    "### The logistic regression model works similarily to the linear regression model by estimating a set of weights per feature, and also an extra weight called the intercept. However, with logistic regression it uses classification which aims to classify an example into one of two classes (target feature is 0 or 1, which is \"Bad\" or \"Good\" in our case).\n",
    "\n",
    "Thus, with classification we can interpret the target feature as the probability of class membership:\n",
    "$target\\_feature = w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n $\n",
    "\n",
    "The probability of class membership is determined as follows:\n",
    "$probability(target=1|descriptive\\_features)=logistic(w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n)$\n",
    "where $logistic(x)$ is defined as: $logistic(x) = \\frac{e ^ x}{1 + e ^ x} = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "From the results of this model, it can be seen that the base value for RiskPerformance is -1.76718641.\n",
    "\n",
    "The weights for the other features are as follows:\n",
    "\n",
    "- For every unit increase in **ExternalRiskEstimate** the RiskPerformance target increases by: **0.02710619**\n",
    "- For every unit increase in **PercentTradesWBalance** the RiskPerformance target increases by: **0.00591177**\n",
    "- For every unit increase in **NetFractionRevolvingBurden** the RiskPerformance target decreases by: **-0.02016581**\n",
    "- For every unit increase in **AverageMInFile** the RiskPerformance target increases by: **0.01673749**\n",
    "- For every unit increase in **MSinceMostRecentDelq** the RiskPerformance target decreases by: **-0.00543609**\n",
    "- For every unit increase in **MSinceOldestTradeOpen** the RiskPerformance target increases by: **0.00274037**\n",
    "- For every unit increase in **NumBank2NatlTradesWHighUtilization** the RiskPerformance target decreases by: **-0.15378619**\n",
    "- For every unit increase in **MaxDelqEver_3** the RiskPerformance target decreases by: **-0.67130334**\n",
    "- For every unit increase in **MaxDelqEver_4** the RiskPerformance target decreases by: **-0.16776911**\n",
    "- For every unit increase in **MaxDelqEver_5** the RiskPerformance target decreases by: **-0.13580476**\n",
    "- For every unit increase in **MaxDelqEver_6** the RiskPerformance target increases by: **0.62118979**\n",
    "- For every unit increase in **MaxDelqEver_7** the RiskPerformance target decreases by: **-0.33580757**\n",
    "- For every unit increase in **MaxDelq2PublicRecLast12M_3** the RiskPerformance target increases by: **0.1982586**\n",
    "- For every unit increase in **MaxDelq2PublicRecLast12M_4** the RiskPerformance target decreases by: **-0.50812457**\n",
    "- For every unit increase in **MaxDelq2PublicRecLast12M_6** the RiskPerformance target increases by: **0.45209493**\n",
    "- For every unit increase in **MaxDelq2PublicRecLast12M_7** the RiskPerformance target decreases by: **-0.28637155**\n",
    "- For every unit increase in **DelqEver_True** the RiskPerformance target decreases by: **-1.43137883**\n",
    "- For every unit increase in **DelqLast12M_True** the RiskPerformance target decreases by: **-1.48081486**\n",
    "\n",
    "### Thus, the model is:\n",
    "\n",
    "\n",
    "\n",
    "**probability(RiskPerformance_Good=1|descriptive_features)=logistic**(-1.76718641 + ExternalRiskEstimate * 0.02710619 + PercentTradesWBalance * 0.00591177 - NetFractionRevolvingBurden * 0.02016581 + AverageMInFile * 0.01673749 - MSinceMostRecentDelq * 0.00543609 + MSinceOldestTradeOpen * 0.00274037 - NumBank2NatlTradesWHighUtilization * 0.15378619 - MaxDelqEver_3 * 0.67130334 - MaxDelqEver_4 * 0.16776911 - MaxDelqEver_5 * 0.13580476 + MaxDelqEver_6 * 0.62118979 - MaxDelqEver_7 * 0.33580757 + MaxDelq2PublicRecLast12M_3 * 0.1982586 - MaxDelq2PublicRecLast12M_4 * 0.50812457 + MaxDelq2PublicRecLast12M_6 * 0.45209493 + MaxDelq2PublicRecLast12M_7 * 0.28637155 - DelqEver_True * 1.43137883 - DelqLast12M_True * 1.48081486)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the predicted probabilities. \n",
    "The output is a pair for each example, \n",
    "The first component is the probability of the negative class (class 0).\n",
    "The second component is the probability of the positive class (class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predicted probabilities for each example. \n",
    "multiple_logreg_predicted_probs = multiple_logreg.predict_proba(X_train)\n",
    "# First 100 for debugging purposes (ensuring no problems).\n",
    "print(multiple_logreg.predict_proba(X_train.head(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the predicted classes of the first 100 training examples, 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multiple_logreg.predict(X_train.head(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logreg_predicted_class = multiple_logreg.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train, multiple_logreg_predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the metrics:\n",
    "\n",
    "The accuracy is 73.8%, which means that the model predicts correctly 73.8% of the time, which is better than a 50/50 choice but still not very accurate.\n",
    "\n",
    "The confusion matrix reveals 231 true negatives, 81 false negatives, 73 false positives, and 203 true positives.\n",
    "\n",
    "The weighted average precision, (correctly predicted positive / predicted positive) is 74%, which shows that when the model predicts a good target outcome, it is correct 74% of the time. Precision is the ratio of correctly predicted positive (here it is a 'good' outcome) observations to the total predicted positive observations.\n",
    "\n",
    "The weighted average recall, (correctly predicted positive / actual positive) is 74%, which shows when the target outcome is good, it predicts that it is good 74% of the time. (When it's actually good, it predicts good 74% of the time.)\n",
    "\n",
    "The weighted average F1-score, (aggregation of Precision and Recall) is 74%, which means there are a reasonably low amount of false positives and false negatives.\n",
    "\n",
    "The MAE is 35.25170068027211, the MSE is 0.2619047619047619, the RMSE is 0.511766315719159, and the R2 is -0.0488324684951813."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated class probabilities on test set\n",
    "print(multiple_logreg.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated classes on test set\n",
    "y_predicted = multiple_logreg.predict(X_test)\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the metrics:\n",
    "\n",
    "In comparison to the training data, the model had an accuracy of 69.4% on the test data versus 73.8%.\n",
    "\n",
    "The weighted average precision is 70% for test versus 74% for training.\n",
    "\n",
    "The weighted average recall is 69% for test verus 74% for training.\n",
    "\n",
    "The weighted average F1-scoreis 69% for test verus 74% for training.\n",
    "\n",
    "The MAE, MSE, and RMSE scores are higher indicating larger error, and the R2 is smaller, indicating a worse fit for the model.\n",
    "\n",
    "The model is not working as well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation using cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = cross_val_score(LogisticRegression(), crossValXdf, crossValYdf, scoring='f1', cv=3)\n",
    "print(\"\\nF1 CV scores:\", f1_scores)\n",
    "print(\"\\nF1 CV mean score:\",f1_scores.mean())\n",
    "\n",
    "acc_scores = cross_val_score(LogisticRegression(), crossValXdf, crossValYdf, scoring='accuracy', cv=3)\n",
    "print(\"\\nAccuracy CV scores:\",acc_scores)\n",
    "print(\"\\nAccuracy CV mean score:\",acc_scores.mean())\n",
    "\n",
    "mean_abs_scores = cross_val_score(LogisticRegression(), crossValXdf, crossValYdf, scoring='neg_mean_absolute_error', cv=3)\n",
    "print(\"\\nneg_mean_absolute_error CV scores:\",mean_abs_scores)\n",
    "print(\"\\nneg_mean_absolute_error CV mean score:\",mean_abs_scores.mean())\n",
    "\n",
    "mean_squared_scores = cross_val_score(LogisticRegression(), crossValXdf, crossValYdf, scoring='neg_mean_squared_error', cv=3)\n",
    "print(\"\\nneg_mean_squared_error CV scores:\",mean_squared_scores)\n",
    "print(\"\\nneg_mean_squared_error CV mean score:\",mean_squared_scores.mean())\n",
    "\n",
    "r2_scores = cross_val_score(LogisticRegression(), crossValXdf, crossValYdf, scoring='r2', cv=3)\n",
    "print(\"\\nr2 CV scores:\",r2_scores)\n",
    "print(\"\\nr2 CV mean score:\",r2_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 CV scores: [0.68592058 0.73764259 0.69565217]\n",
    "\n",
    "F1 CV mean score: 0.7064051123605676\n",
    "\n",
    "Accuracy CV scores: [0.69039146 0.75357143 0.69892473]\n",
    "\n",
    "Accuracy CV mean score: 0.7142958729429858\n",
    "\n",
    "neg_mean_absolute_error CV scores: [-36.46619217 -34.71785714 -34.89605735]\n",
    "\n",
    "neg_mean_absolute_error CV mean score: -35.36003555378196\n",
    "\n",
    "neg_mean_squared_error CV scores: [-0.30960854 -0.24642857 -0.30107527]\n",
    "\n",
    "neg_mean_squared_error CV mean score: -0.2857041270570142\n",
    "\n",
    "r2 CV scores: [-0.24033486  0.01302682 -0.20617602]\n",
    "\n",
    "r2 CV mean score: -0.14449468398311816\n",
    "\n",
    "\n",
    "The cross validated model has an accuracy of 71.4%, whereas the test data model is 69.4%\n",
    "\n",
    "The cross validated model has an MAE of 35.36 whereas the test data model is 35.58\n",
    "The cross validated model has an RMSE of 0.2857, whereas the test data model is 0.5527\n",
    "The cross validated model has an R2 of -0.14449, whereas the test data model is -0.225\n",
    "The cross validated model has an R2 of -0.14449, whereas the test data model is -0.225\n",
    "The cross validated model has an F1 of 0.7064, whereas the test data model is 0.69\n",
    "\n",
    "The cross validated model outperforms the test data model in every metric, and it is clear that cross validating a model provides for a better model, as there are more datasets.\n",
    "\n",
    "\n",
    "### Logistic regression evaluation\n",
    "\n",
    "In linear regression, accuracy scores of 70.6% for the test model and 72.26% for the cross validation model were obtained. For the logistic regression model, test data model was 69.4% accurate, and the cross validated model was 71.4%. Thus, based on accuracy, the linear regression model is the better of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, the Random Forest predictive model will be evaluated. This model will interpret the target feature as the probability of class membership, and will create these estimations probabilities based on the mean predicted class probabilities of the trees in the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the random forest with 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance from the RFC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting random forests\n",
    "\n",
    "Interpreting random forest is not as straightforward as linear and logistic regression models. The random forest model is somewhat similar to logistic regression in that it predicts classes, and classifies class membership in a similar way: \n",
    "\n",
    "$probability(target=1|descriptive\\_features)$\n",
    "\n",
    "A random forest is made up of many decision trees, and for a single decision tree this probability is estimated as the proportion of examples from the positive class at the leaf node that contains the test example. This probability is estimated as the mean predicted class probabilities of the trees in the forest. \n",
    "\n",
    "So for example if we use 3 trees, with probability for class 1 on given example being 0.6, 0.7 and 0.8, \n",
    "then the probability $probability(target=1|descriptive\\_features) = \\frac{0.6 + 0.7 + 0.8}{3} = 0.7$.\n",
    "\n",
    "As it is not possible to look at each of the 100 trees and understand how the probability is estimated, the feature importance table above is what can be used.\n",
    "\n",
    "From looking at the table, it can be seen that:\n",
    "\n",
    "ExternalRiskEstimate is the most important feature, with an importance of 0.184277\n",
    "\n",
    "The following 4 features are the next most important in order, all with an importance over 0.1:\n",
    "\n",
    "AverageMInFile\n",
    "NetFractionRevolvingBurden\n",
    "MSinceOldestTradeOpen\n",
    "PercentTradesWBalance\n",
    "\n",
    "The following are the next most important in order, with an importance over 0.05\n",
    "MSinceMostRecentDelq\n",
    "NumBank2NatlTradesWHighUtilization\n",
    "\n",
    "The rest of the features have extremely low importance.\n",
    "\n",
    "The importance is like a weighting, with the high importance values being more useful in obtaining a prediction for the target feature. In this model, the importance points to a weighting hierarchy which determines which features are more useful for making the prediction, with that hierarchy displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdf = pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})\n",
    "sortplotdf = plotdf.sort_values(by=['importance'], ascending=False)\n",
    "ax = sortplotdf.plot.bar(x='feature', y='importance', figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the training dataset using the trained random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted probabilities for all examples. The output is a pair for each example, the first component is the probability of the negative class (class 0) and the second component is the probability of the positive class (class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc.predict_proba(X_train.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model to make predictions on the training data, and print the first 100 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions_100 = rfc.predict(X_train.head(100))\n",
    "rfc_predictions = rfc.predict(X_train)\n",
    "rfc_predictions_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the actual versus predicted class for the first 100 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_true_vs_rfc_train_predicted = pd.DataFrame({'ActualClass': y_train.head(100), 'PredictedClass': rfc_predictions_100})\n",
    "df_true_vs_rfc_train_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train, rfc_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the metrics\n",
    "\n",
    "Here, everything is a perfect score. This is evidence that the model is overfitting, and has learned the training data exactly. This is highly undesirable, and other parameters will have to be used to remedy this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_predictions_test = rfc.predict(X_test)\n",
    "df_true_vs_rfc_predicted_test = pd.DataFrame({'ActualClass': y_test, 'PredictedClass': rfc_predictions_test})\n",
    "df_true_vs_rfc_predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test, rfc_predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the metrics\n",
    "\n",
    "There is a massive difference between the training data metrics and the test metrics, with the test results being far less accurate. This is expected, as the model has overfit on the training data, which leads to poor performance on new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = cross_val_score(RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1), crossValXdf, crossValYdf, scoring='f1', cv=3)\n",
    "print(\"\\nF1 CV scores:\", f1_scores)\n",
    "print(\"\\nF1 CV mean score:\",f1_scores.mean())\n",
    "\n",
    "acc_scores = cross_val_score(RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1), crossValXdf, crossValYdf, scoring='accuracy', cv=3)\n",
    "print(\"\\nAccuracy CV scores:\",acc_scores)\n",
    "print(\"\\nAccuracy CV mean score:\",acc_scores.mean())\n",
    "\n",
    "mean_abs_scores = cross_val_score(RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1), crossValXdf, crossValYdf, scoring='neg_mean_absolute_error', cv=3)\n",
    "print(\"\\nneg_mean_absolute_error CV scores:\",mean_abs_scores)\n",
    "print(\"\\nneg_mean_absolute_error CV mean score:\",mean_abs_scores.mean())\n",
    "\n",
    "mean_squared_scores = cross_val_score(RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1), crossValXdf, crossValYdf, scoring='neg_mean_squared_error', cv=3)\n",
    "print(\"\\nneg_mean_squared_error CV scores:\",mean_squared_scores)\n",
    "print(\"\\nneg_mean_squared_error CV mean score:\",mean_squared_scores.mean())\n",
    "\n",
    "r2_scores = cross_val_score(RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1), crossValXdf, crossValYdf, scoring='r2', cv=3)\n",
    "print(\"\\nr2 CV scores:\",r2_scores)\n",
    "print(\"\\nr2 CV mean score:\",r2_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the out of bag score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis:\n",
    "\n",
    "Using cross-validation, with uses more datasets, it can be seen that there are quite respectable metrics scores in comparison to the other predicitive models tested, and better than the test dataset model. However, it cannot be fully determined whether or not the random forest parameters are allowing for overfitting in some of the cross validation datasets too. Thus, with the level of overfitting witnessed, it is best to avoid this model unless parameter changes are made and the training data is seen not to overfit so heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis thus far\n",
    "\n",
    "##### So far, the linear regression model has the best metrics. Logistic regression is close behind; within a few percentage points. Random Forest however, was overfitting in the training dataset, and thus the metrics are not reliable. \n",
    "\n",
    "The accuracy results thus far are:\n",
    "\n",
    "**Linear Regression**, \n",
    "70.6% for the test model and 72.26% for the cross validation model\n",
    "\n",
    "**Logistic Regression** \n",
    "69.4% for the test model and 71.4% for the cross validation model \n",
    "\n",
    "**Random Forest**\n",
    "69.4% for the test model and 71.4% for the cross validation model \n",
    "\n",
    "What's interesting is that the overfit random forest model obtains the same accuracy as the logistic regression model, but still, both of these models are outperformed very slightly by the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive models thus far have been relatively successful in that they outperform a simple model which just always predicts the majority class. This can be shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossValYdf.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "436/840"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, 51.9% of the time, the target value will be 0, or \"Bad\". In all of the predictive models evaluated above, the accuracy rating were far higher than this. Thus, in helping the company to solve their RiskPerformance target prediction problem, any of the models would do a better job than the simple majority class model.\n",
    "\n",
    "However, just because these models were run once with certain parameters does not mean they cannot improve. Thus, seeing as the accuracy for all 3 were roughly the same, and the random forest model was overfitting, attempted improvements to all three will be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with one descriptive feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was evident from the boxplot correlation thatn ExternalRiskEstimate was the feature with the highest correlation with the target feature. Thus, instead of doing multiple linear regression, linear regression with one feature, ExternalRiskEstimate, will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ExternalRiskEstimate']\n",
    "linreg = LinearRegression().fit(X_train[features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the estimated linear regression coefficients.\n",
    "print(\"Features: \\n\", features)\n",
    "print(\"Coeficients: \\n\", linreg.coef_)\n",
    "print(\"\\nIntercept: \\n\", linreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A unit increase in ExternalRiskEstimate leads to a 0.02455584 increase in the target feature, with a bias of  -1.286238303310054"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linreg_predictions = linreg.predict(X_train[features])\n",
    "\n",
    "# Set less than 0.5 to 0, and 0.5 and greater to 1.\n",
    "preddf = pd.DataFrame(linreg_predictions, columns=['Predicted'])\n",
    "preddf.loc[preddf.Predicted < 0.5, 'predicted_Yes_No'] = 0 \n",
    "preddf.loc[preddf.Predicted >= 0.5, 'predicted_Yes_No'] = 1 \n",
    "preddf = preddf.drop(\"Predicted\", axis=1)\n",
    "\n",
    "# Reset the indices to allow the rows to align correctly.\n",
    "y_train.reset_index(drop=True)\n",
    "preddf.reset_index(drop=True)\n",
    "\n",
    "# Show the actual vs predicted values for the first 100 training examples.\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train.head(100), preddf.head(100)], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train, preddf['predicted_Yes_No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predicted scores for each example. \n",
    "linreg_predictions_test = linreg.predict(X_test[features])\n",
    "\n",
    "preddfTest = pd.DataFrame(linreg_predictions_test, columns=['Predicted'])\n",
    "preddfTest.loc[preddfTest.Predicted < 0.5, 'predicted_Yes_No'] = 0 \n",
    "preddfTest.loc[preddfTest.Predicted >= 0.5, 'predicted_Yes_No'] = 1 \n",
    "preddfTest = preddfTest.drop(\"Predicted\", axis=1)\n",
    "\n",
    "test_actual_vs_predicted_multiplelinreg = pd.concat([y_test.reset_index(drop=True), preddfTest.reset_index(drop=True)], axis=1)\n",
    "print(test_actual_vs_predicted_multiplelinreg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test, preddfTest['predicted_Yes_No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the test results actually outperform the training results very slightly. Both have .71 for their precision, recall, and f1 scores, but with test having an accuracy of 71.03% versus 70.4% against training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(linreg, crossValXdf, crossValYdf, cv=3)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply thresholding\n",
    "for i, predict in enumerate(predictions):\n",
    "     predictions[i] = 0 if (predict < 0.5) else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(predictions, crossValYdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validated results outperform the test results slightly:\n",
    "\n",
    "Accuracy\n",
    "- Test: 71.03%   CV: 72.26%\n",
    "\n",
    "precision, recall, f1-score:\n",
    "- Test: .71      CV: .72\n",
    "\n",
    "So far, this is actually the accuracy model evaluated, beating the previous high score of 70.6% for the test results and 72.26% for the cross validation results for the multiple linear regression model. Thus, the attempted improvements were a success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with changed parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was seen that the random forest model created earlier was overfitting heavily. Thus, an attempt to remedy that will be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RF with 100 trees\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training dataset\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance features can often be more valuable in selecting features than the correlations. Thus, the top 5 highest importance features will be used for this model. Features with importance over 0.1 in the table above:\n",
    "'ExternalRiskEstimate', 'NetFractionRevolvingBurden', 'PercentTradesWBalance', 'AverageMInFile', 'MSinceOldestTradeOpen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "cont_features_improved = ['ExternalRiskEstimate', 'NetFractionRevolvingBurden', 'PercentTradesWBalance', 'AverageMInFile', 'MSinceOldestTradeOpen']\n",
    "\n",
    "RiskPerformance_dummies_improved = pd.get_dummies(traindf['RiskPerformance'], prefix='RiskPerformance', drop_first=True)\n",
    "\n",
    "# Continuous and categorical features combined \n",
    "features_improved = cont_features_improved + RiskPerformance_dummies_improved.columns.values.tolist()\n",
    "print(\"Features: \", features_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_improved = pd.concat([traindf, RiskPerformance_dummies_improved], axis=1)\n",
    "# Keep the features chosen.\n",
    "feat_to_keep = features_improved\n",
    "\n",
    "traindf_improved = traindf_improved.loc[:, feat_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_improved.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created updated dataframes with the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_improved = traindf_improved[[x for x in traindf_improved[features_improved] if x not in ['RiskPerformance_Good']]]\n",
    "y_train_improved = traindf_improved.RiskPerformance_Good\n",
    "\n",
    "print(\"\\nDescriptive features in X:\\n\", X_train_improved)\n",
    "print(\"\\nTarget feature in y:\\n\", y_train_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training dataset\n",
    "rfc_improved.fit(X_train_improved, y_train_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature': X_train_improved.columns, 'importance':rfc_improved.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All features have relatively high importance compared to the first feature importance table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_improved.predict_proba(X_train_improved.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions_100_improved = rfc_improved.predict(X_train_improved.head(100))\n",
    "rfc_predictions_improved = rfc_improved.predict(X_train_improved)\n",
    "rfc_predictions_100_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_true_vs_rfc_train_predicted = pd.DataFrame({'ActualClass': y_train_improved.head(100), 'PredictedClass': rfc_predictions_100_improved})\n",
    "df_true_vs_rfc_train_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train, rfc_predictions_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the model is still overfitting, even with the changes in features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "cont_features_improved = ['ExternalRiskEstimate', 'NetFractionRevolvingBurden', 'PercentTradesWBalance', 'AverageMInFile', 'MSinceOldestTradeOpen']\n",
    "\n",
    "RiskPerformance_dummies_improved = pd.get_dummies(testdf['RiskPerformance'], prefix='RiskPerformance', drop_first=True)\n",
    "\n",
    "# Continuous and categorical features combined \n",
    "features_improved = cont_features_improved + RiskPerformance_dummies_improved.columns.values.tolist()\n",
    "print(\"Features: \", features_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_improved = pd.concat([testdf, RiskPerformance_dummies_improved], axis=1)\n",
    "# Keep the features chosen.\n",
    "feat_to_keep = features_improved\n",
    "\n",
    "testdf_improved = testdf_improved.loc[:, feat_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_improved = testdf_improved[[x for x in testdf_improved[features_improved] if x not in ['RiskPerformance_Good']]]\n",
    "y_test_improved = testdf_improved.RiskPerformance_Good\n",
    "\n",
    "print(\"\\nDescriptive features in X:\\n\", X_test_improved)\n",
    "print(\"\\nTarget feature in y:\\n\", y_test_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_predictions_test_impr = rfc_improved.predict(X_test_improved)\n",
    "df_true_vs_rfc_predicted_test_impr = pd.DataFrame({'ActualClass': y_test_improved, 'PredictedClass': rfc_predictions_test_impr})\n",
    "df_true_vs_rfc_predicted_test_impr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test, rfc_predictions_test_impr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1), crossValXdf, crossValYdf, scoring='accuracy', cv=3)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: \n",
    "Compared to the previous random forest model, this model has a minutely higher accuracy, at 69.8% versus 69.4%. The out of bag score remains unchanged. While improvements have been made, the overfitting is not acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting again was a rather disappointing outcome, and so further parameters will be changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previously, 100 estimators were being used. Now, an attempt to find the most accurate number will estimators will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,110,10):\n",
    "    rfc_improved = RandomForestClassifier(n_estimators=i, max_features='auto', oob_score=True, random_state=1)\n",
    "    rfc_improved.fit(X_train_improved, y_train_improved) \n",
    "    rfc_predictions = rfc_improved.predict(X_test_improved)\n",
    "    print(\"Accuracy: \", i, metrics.accuracy_score(y_test_improved, rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10, 60, 80, and 90 estimators all perform best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the previous random forest models, the min_samples_leaf parameter was not specified. Thus, the best combination of estimator size and min_samples_leaf size will be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,5,10,50,100,200,500]:\n",
    "    for j in [10,60,80,90]:\n",
    "        rfc_improved = RandomForestClassifier(n_estimators= 60, max_features='auto', oob_score=True, random_state=1, min_samples_leaf=i)\n",
    "        rfc_improved.fit(X_train_improved, y_train_improved) \n",
    "        rfc_predictions = rfc_improved.predict(X_test_improved)\n",
    "        print(\"estimators: \", i,\" min_samples_leaf: \", j,\" Accuracy: \", metrics.accuracy_score(y_test_improved, rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combinations:\n",
    "    \n",
    "estimators:  10  min_samples_leaf:  10  Accuracy:  0.7301587301587301\n",
    "estimators:  10  min_samples_leaf:  60  Accuracy:  0.7301587301587301\n",
    "estimators:  10  min_samples_leaf:  80  Accuracy:  0.7301587301587301\n",
    "estimators:  10  min_samples_leaf:  90  Accuracy:  0.7301587301587301\n",
    "\n",
    "Are all equal, and thus estimators:  10  min_samples_leaf:  10 will be chosen, as it should use less computation resources than the other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and fit the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved2 = RandomForestClassifier(n_estimators= 10, max_features='auto', oob_score=True, random_state=1, min_samples_leaf=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training dataset\n",
    "rfc_improved2.fit(X_train_improved, y_train_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_improved2.predict_proba(X_train_improved.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions_100_improved2 = rfc_improved2.predict(X_train_improved.head(100))\n",
    "rfc_predictions_improved2 = rfc_improved2.predict(X_train_improved)\n",
    "rfc_predictions_100_improved2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train_improved, rfc_predictions_improved2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_predictions_test_impr = rfc_improved2.predict(X_test_improved)\n",
    "df_true_vs_rfc_predicted_test_impr = pd.DataFrame({'ActualClass': y_test_improved, 'PredictedClass': rfc_predictions_test_impr})\n",
    "df_true_vs_rfc_predicted_test_impr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test_improved, rfc_predictions_test_impr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(RandomForestClassifier(n_estimators=10, max_features='auto', oob_score=True, random_state=1, min_samples_leaf=10), crossValXdf, crossValYdf, scoring='accuracy', cv=3)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved2.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "The changes to the parameters were a huge success. The model is no longer massively overfitting, and there is a 4% jump in accuracy in the test results over the first random forest that was created. \n",
    "\n",
    "The training results here are 78.4% accuracy vs the test results of 73.4%. \n",
    "\n",
    "Precision, recall and f1-score are .78 across the board for training, and .74,.73,.73 for test.\n",
    "\n",
    "The cross validation score for the altered parameter random forest model saw an increase from 71.430% in previous models, to 72.857%, accuracy wise.\n",
    "\n",
    "The OOB score improved from 0.7295918367346939 to 0.7329931972789115.\n",
    "\n",
    "This model is now the most accurate of those tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These features worked very well with the random forest model, so now an evaluation will be done to see if using these features can improve the linear regression and logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_linreg = LinearRegression().fit(X_train_improved, y_train_improved)\n",
    "\n",
    "# Print the intercept\n",
    "print(\"\\nIntercept: \\n\", multiple_linreg.intercept_)\n",
    "print()\n",
    "# Print the features and coefficients\n",
    "print(\"Features and coefficients:\", list(zip(features_improved, multiple_linreg.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a unit increase in:\n",
    "- 'ExternalRiskEstimate' the target feature increases by: 0.018671818287282132\n",
    "- 'NetFractionRevolvingBurden' the target feature decreases by: -0.0027070379416195035\n",
    "- 'PercentTradesWBalance' the target feature increases by: 0.001682642850419566\n",
    "- 'AverageMInFile' the target feature increases by: 0.0022151730368995\n",
    "- 'MSinceOldestTradeOpen' the target feature increases by: 0.0005034003076700728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict on the training descriptive features.\n",
    "multiple_linreg_predictions = multiple_linreg.predict(X_train_improved)\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "\n",
    "y_train_improved.reset_index(drop=True)\n",
    "noThresPredDf_imp = pd.DataFrame(multiple_linreg_predictions, columns=['Predicted'])\n",
    "noThresPredDf_imp.reset_index(drop=True)\n",
    "\n",
    "# Show the actual vs predicted values for the first 100 training examples.\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train_improved.head(100), noThresPredDf_imp.head(100)], axis=1)\n",
    "\n",
    "print(actual_vs_predicted_multiplelinreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set less than 0.5 to 0, and 0.5 and greater to 1.\n",
    "preddf_imp = pd.DataFrame(multiple_linreg_predictions, columns=['Predicted'])\n",
    "preddf_imp.loc[preddf_imp.Predicted < 0.5, 'predicted_Yes_No'] = 0 \n",
    "preddf_imp.loc[preddf_imp.Predicted >= 0.5, 'predicted_Yes_No'] = 1 \n",
    "preddf_imp = preddf_imp.drop(\"Predicted\", axis=1)\n",
    "\n",
    "# Reset the indices to allow the rows to align correctly.\n",
    "y_train_improved.reset_index(drop=True)\n",
    "preddf_imp.reset_index(drop=True)\n",
    "\n",
    "# Show the actual vs predicted values for the first 100 training examples.\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train_improved.head(100), preddf_imp.head(100)], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train, preddf_imp['predicted_Yes_No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predictions = multiple_linreg.predict(X_test_improved)\n",
    "print(\"Actual values of test:\\n\", y_test)\n",
    "print(\"Predictions on test:\", test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preddf_imp = pd.DataFrame(test_predictions, columns=['Predicted'])\n",
    "test_preddf_imp.loc[test_preddf_imp.Predicted < 0.5, 'predicted_Yes_No'] = 0 \n",
    "test_preddf_imp.loc[test_preddf_imp.Predicted >= 0.5, 'predicted_Yes_No'] = 1 \n",
    "test_preddf_imp = test_preddf_imp.drop(\"Predicted\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test, test_preddf_imp['predicted_Yes_No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "With an accuracy of 74.65% in training and 73.015% in test, this model is well fit. It is also an improvement on the single descriptive feature linear regression model, with the test results here getting 73.015% versus 71.03%.\n",
    "\n",
    "This model is therefore now the second best model accuracy wise, after the improved random forest model (73.4% in test for the random forest, 73.015% for this model in test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logreg_imp = LogisticRegression().fit(X_train_improved, y_train_improved)\n",
    "\n",
    "print(\"Features: \\n\", features_improved)\n",
    "print(\"Coefficients: \\n\", multiple_logreg_imp.coef_)\n",
    "print(\"\\nIntercept: \\n\", multiple_logreg_imp.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a unit increase in:\n",
    "- 'ExternalRiskEstimate' the target feature increases by: 4.57080607e-02\n",
    "- 'NetFractionRevolvingBurden' the target feature decreases by: -2.03006484e-02\n",
    "- 'PercentTradesWBalance' the target feature increases by: 04.45268159e-05\n",
    "- 'AverageMInFile' the target feature increases by: 1.22015790e-02\n",
    "- 'MSinceOldestTradeOpen' the target feature increases by: 2.05936840e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiple_logreg_predicted_probs = multiple_logreg_imp.predict_proba(X_train_improved)\n",
    "print(multiple_logreg_imp.predict_proba(X_train_improved.head(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multiple_logreg_imp.predict(X_train_improved.head(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logreg_predicted_class = multiple_logreg_imp.predict(X_train_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logreg_imp.score(X_train_improved, y_train_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train_improved, multiple_logreg_predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_imp = multiple_logreg_imp.predict(X_test_improved)\n",
    "print(y_predicted_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multiple_logreg_imp.predict_proba(X_test_improved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test_improved, y_predicted_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The training data results show an accuracy of 73.29%, and the test results show an accuracy of 72.61%. This is a large increase over the first Logistic Regression model which had 69.4% for the test result accuracy. The improvements to the model worked well, but it is still behind both the improved multiple linear regression model and the improved random forest model when it comes to accuracy.\n",
    "\n",
    "(73.4% in test for the random forest, 73.015% for the multiple linear regression model, and 72.61% in this, the logistic regression model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the data put into a predictive model can often lead to improvements, and thus it will be attempted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_norm = traindf[['ExternalRiskEstimate', 'NetFractionRevolvingBurden', 'PercentTradesWBalance', 'AverageMInFile', 'MSinceOldestTradeOpen']]\n",
    "norm_features_df = (features_to_norm - features_to_norm.mean()) / (features_to_norm.std())\n",
    "print(norm_features_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RiskPerformance_dummies = pd.get_dummies(traindf['RiskPerformance'], prefix='RiskPerformance', drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new dataframes with normalised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_df_train = pd.concat([norm_features_df, RiskPerformance_dummies], axis=1)\n",
    "norm_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_norm_test = testdf[['ExternalRiskEstimate', 'NetFractionRevolvingBurden', 'PercentTradesWBalance', 'AverageMInFile', 'MSinceOldestTradeOpen']]\n",
    "norm_features_test_df = (features_to_norm_test - features_to_norm_test.mean()) / (features_to_norm_test.std())\n",
    "print(norm_features_test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RiskPerformance_dummies = pd.get_dummies(testdf['RiskPerformance'], prefix='RiskPerformance', drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_df_test = pd.concat([norm_features_test_df, RiskPerformance_dummies], axis=1)\n",
    "norm_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ExternalRiskEstimate', 'NetFractionRevolvingBurden', 'PercentTradesWBalance', 'AverageMInFile', 'MSinceOldestTradeOpen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = norm_df_train[[x for x in norm_df_train[features] if x not in ['RiskPerformance_Good']]]\n",
    "y_train_norm = norm_df_train.RiskPerformance_Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = norm_df_test[[x for x in norm_df_test[features] if x not in ['RiskPerformance_Good']]]\n",
    "y_test_norm = norm_df_test.RiskPerformance_Good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved2.fit(X_train_norm, y_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_improved2.predict_proba(X_train_norm.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions_100_improved2 = rfc_improved2.predict(X_train_norm.head(100))\n",
    "rfc_predictions_improved2 = rfc_improved2.predict(X_train_norm)\n",
    "rfc_predictions_100_improved2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train_norm, rfc_predictions_improved2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_predictions_test_impr = rfc_improved2.predict(X_test_norm)\n",
    "df_true_vs_rfc_predicted_test_impr = pd.DataFrame({'ActualClass': y_test_norm, 'PredictedClass': rfc_predictions_test_impr})\n",
    "df_true_vs_rfc_predicted_test_impr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test_norm, rfc_predictions_test_impr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved2.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(RandomForestClassifier(n_estimators=10, max_features='auto', oob_score=True, random_state=1, min_samples_leaf=10), crossValXdf, crossValYdf, scoring='accuracy', cv=3)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The normalisation did not result in an improvement, and in fact saw a decrease in metric performance compared to the improved random forest model. For example, the test result accuracy here is 73% versus 73.4% in the improved model from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A final attempt will be made to improve the random forest, by only using the top 3 features ranked by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "cont_features_improved = ['ExternalRiskEstimate', 'NetFractionRevolvingBurden', 'MSinceOldestTradeOpen']\n",
    "\n",
    "RiskPerformance_dummies_improved = pd.get_dummies(traindf['RiskPerformance'], prefix='RiskPerformance', drop_first=True)\n",
    "\n",
    "# Continuous and categorical features combined \n",
    "features_improved = cont_features_improved + RiskPerformance_dummies_improved.columns.values.tolist()\n",
    "print(\"Features: \", features_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_improved = pd.concat([traindf, RiskPerformance_dummies_improved], axis=1)\n",
    "# Keep the features chosen.\n",
    "feat_to_keep = features_improved\n",
    "\n",
    "traindf_improved = traindf_improved.loc[:, feat_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_improved.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_improved = traindf_improved[[x for x in traindf_improved[features_improved] if x not in ['RiskPerformance_Good']]]\n",
    "y_train_improved = traindf_improved.RiskPerformance_Good\n",
    "\n",
    "print(\"\\nDescriptive features in X:\\n\", X_train_improved)\n",
    "print(\"\\nTarget feature in y:\\n\", y_train_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_improved = testdf_improved[[x for x in testdf_improved[features_improved] if x not in ['RiskPerformance_Good']]]\n",
    "y_test_improved = testdf_improved.RiskPerformance_Good\n",
    "\n",
    "print(\"\\nDescriptive features in X:\\n\", X_test_improved)\n",
    "print(\"\\nTarget feature in y:\\n\", y_test_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved2 = RandomForestClassifier(n_estimators= 10, max_features='auto', oob_score=True, random_state=1, min_samples_leaf=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved2.fit(X_train_improved, y_train_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_improved2.predict_proba(X_train_improved.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions_100_improved2 = rfc_improved2.predict(X_train_improved.head(100))\n",
    "rfc_predictions_improved2 = rfc_improved2.predict(X_train_improved)\n",
    "rfc_predictions_100_improved2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_train_improved, rfc_predictions_improved2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_predictions_test_impr = rfc_improved2.predict(X_test_improved)\n",
    "df_true_vs_rfc_predicted_test_impr = pd.DataFrame({'ActualClass': y_test_improved, 'PredictedClass': rfc_predictions_test_impr})\n",
    "df_true_vs_rfc_predicted_test_impr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y_test_improved, rfc_predictions_test_impr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_improved2.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(RandomForestClassifier(n_estimators=10, max_features='auto', oob_score=True, random_state=1, min_samples_leaf=10), crossValXdf, crossValYdf, scoring='accuracy', cv=3)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Choosing the top 3 features by importance did not yield a better result. The test results accuracy is 71.42% versus 73.4% for the best random forest model, and the oob score has reduced from 0.7329931972789115 to 0.70578231292517. This shows that reducing the random forest model to ever smaller numbers of important features does not guarantee an improvement in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Many different models were tested throughout this notebook, and while none reached a very high accuracy, they were all a large improvement over a simple predicitive model which would have just made a prediction based on the majority class. \n",
    "\n",
    "For the company and their business problem, it might be tempting to say that the model with the highest accuracy is the best fit. However, accuracy alone is the only metric which a predicitive model must be judged by. Resource usage such as CPU load, disk size, and ram usagemay be a concern to the company. If this is the case, then the best model would be the linear regression model with one descriptive feature, as it was simple but also one of the highest accuracy models (72.61% in test).\n",
    "\n",
    "Explainability is another factor which may be important, especially for a credit company. Sometimes, sacrificing a little accuracy for better interpretation of the predicitive model is desirable, as if an entity wishes to know why the predicted risk performance was bad, such as a returning customer, the company would like to have good reasoning behind the decision. If this is the case, then the random forest model would not be a good fit, as interpreting it is a difficult task. The improved multiple linear regression model might be the best trade off here for accuracy and interpretability (73.015% in test).\n",
    "\n",
    "However, if accuracy is the main priority, then the improved random forest model (73.4% in test) would be the best choice for the company."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
